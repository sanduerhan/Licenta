{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanduerhan/Licenta/blob/main/code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OH7f6H-lQfOA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Import"
      ],
      "metadata": {
        "id": "EOmssdxm3o5N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "collapsed": true,
        "id": "ZX3Ibl5BQUFC"
      },
      "outputs": [],
      "source": [
        "#importing the dataset\n",
        "import pandas as pd\n",
        "col_list = [\"Column2\", \"Column3\"]\n",
        "dataset = pd.read_excel(\"Training.data.xlsx\", usecols=col_list)\n",
        "testset = pd.read_excel(\"Dev.xlsx\", usecols=col_list)\n",
        "train_x = dataset[\"Column2\"]\n",
        "train_y = dataset[\"Column3\"]\n",
        "test_x = testset[\"Column2\"]\n",
        "test_y = testset[\"Column3\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PreProcessing"
      ],
      "metadata": {
        "id": "xHx-Yr8O3pvF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "waiting mind breakdown new year feeling isnt anymore dont know anyone else im little bit worried ill go back depressed days time something last year tried breakdowns start mere days later broke crying wasnt entire year december ok month wait weird way act feel feels bit normal\n"
          ]
        }
      ],
      "source": [
        "# cleaning the text : deleting the emojis, the existing emails, the punctuation, the present digits,\n",
        "# the hyperlinks and the stopwords (a,the,is, etc)\n",
        "import nltk\n",
        "import openpyxl\n",
        "from nltk.corpus import stopwords, words, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re, string\n",
        "from string import punctuation, digits\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "def delete_emoji(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               u\"\\U0001f926-\\U0001f937\"\n",
        "                               u\"\\U00010000-\\U0010ffff\"\n",
        "                               u\"\\u2640-\\u2642\"\n",
        "                               u\"\\u2600-\\u2B55\"\n",
        "                               u\"\\u200d\"\n",
        "                               u\"\\u23cf\"\n",
        "                               u\"\\u23e9\"\n",
        "                               u\"\\u231a\"\n",
        "                               u\"\\ufe0f\"  # dingbats\n",
        "                               u\"\\u3030\"\n",
        "                               \"]+\", re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)  # no emoji\n",
        "\n",
        "\n",
        "def delete_digits(text):\n",
        "    text = text.lower()\n",
        "    clean = text.translate(str.maketrans('', '', digits))\n",
        "    return clean\n",
        "\n",
        "\n",
        "def delete_punctuation(text):\n",
        "    clean = text.translate(str.maketrans('', '', punctuation + '’“”'))\n",
        "    return clean\n",
        "\n",
        "\n",
        "stop = set(stopwords.words('english'))\n",
        "punct = list(string.punctuation)\n",
        "stop.update(punct)\n",
        "\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    final_text = []\n",
        "    for i in text.split():\n",
        "        if i.strip().lower() not in stop:\n",
        "            final_text.append(i.strip())\n",
        "    return \" \".join(final_text)\n",
        "\n",
        "\n",
        "def remove_hyperlinks(text):\n",
        "    for word in text:\n",
        "        if re.match(r'^http', word):\n",
        "            text.remove(word)\n",
        "    return \" \".join(text)\n",
        "\n",
        "\n",
        "def remove_emails(text):\n",
        "    text = text.split()\n",
        "    for i in text:\n",
        "        if '@' in i.strip().lower():\n",
        "            text.remove(i)\n",
        "    return \" \".join(text)\n",
        "\n",
        "\n",
        "def denoise_text(text):\n",
        "    text = remove_emails(text)\n",
        "    text = delete_punctuation(text)\n",
        "    text = delete_emoji(text)\n",
        "    text = delete_digits(text)\n",
        "    text = remove_stopwords(text)\n",
        "    text = text.split()\n",
        "    text = remove_hyperlinks(text)\n",
        "    return text\n",
        "\n",
        "\n",
        "train_x = train_x.apply(denoise_text)\n",
        "test_x = test_x.apply(denoise_text)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fySiVht3QUFH",
        "outputId": "08c064b3-1e9a-484f-f00e-8e7496d66d82"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatization"
      ],
      "metadata": {
        "id": "cfS5G2jQ4GGh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# lemmatization : crying -> cry, days -> day\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "def get_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "\n",
        "def lemm(text):\n",
        "    text = text.split()\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    sar_list_lemmatizer = [lemmatizer.lemmatize(word, get_pos(word)) for word in text]\n",
        "    return \" \".join(sar_list_lemmatizer)\n",
        "\n",
        "train_x = train_x.apply(lemm)\n",
        "test_x = test_x.apply(lemm)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oSwfDyzQUFb",
        "outputId": "9f3c4d7f-0c9d-448a-c6e2-6348c447139b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Count Vectorizer"
      ],
      "metadata": {
        "id": "IMHPbMBI4VTQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "outputs": [],
      "source": [
        "# CountVectorizer reprezentation for the user tweets\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "vectorizer = CountVectorizer()\n",
        "x_train_cv = vectorizer.fit_transform(train_x)\n",
        "\n",
        "x_test_cv = vectorizer.transform(test_x)\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "8blt8KSfQUFd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data imbalance handling"
      ],
      "metadata": {
        "id": "7MBdfMwk4bR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "sm = SMOTE(random_state = 42)\n",
        "\n",
        "res_x, res_y = sm.fit_resample(x_train_cv, train_y)\n",
        "# print(\"After OverSampling, counts of label '1': {}\".format(sum(res_y == \"severe\")))"
      ],
      "metadata": {
        "id": "w-x13nL-a0EM"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os = RandomOverSampler()\n",
        "res_x2, res_y2 = os.fit_resample(x_train_cv, train_y)"
      ],
      "metadata": {
        "id": "020ZvbDV8Yk7"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tf-Idf "
      ],
      "metadata": {
        "id": "2abJM4PN4yyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tf-idf reprezentation for the tweets\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_df = 0.5)\n",
        "train_x_tf = vectorizer.fit_transform(train_x)\n",
        "test_x_tf = vectorizer.transform(test_x)\n",
        "\n",
        "res_tfx, res_tfy = sm.fit_resample(train_x_tf, train_y)"
      ],
      "metadata": {
        "id": "WkoI7tMeRnXz"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes"
      ],
      "metadata": {
        "id": "KsTCPqZS4jzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "naive_bayes_classifier = MultinomialNB()\n",
        "naive_bayes_classifier.fit(res_x, res_y)\n",
        "pred_y = naive_bayes_classifier.predict(x_test_cv)\n",
        "\n",
        "score1 = metrics.accuracy_score(test_y, pred_y)\n",
        "print(\"Accuracy \" + str(score1))\n",
        "print(metrics.classification_report(test_y, pred_y))"
      ],
      "metadata": {
        "id": "uDe2Ts7ZgnZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "naive_bayes_classifier = MultinomialNB()\n",
        "naive_bayes_classifier.fit(res_tfx, res_tfy)\n",
        "pred_y = naive_bayes_classifier.predict(test_x_tf)\n",
        "score1 = metrics.accuracy_score(test_y, pred_y)\n",
        "print(\"Accuracy \" + str(score1))\n",
        "print(metrics.classification_report(test_y, pred_y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVkenp6V8Qce",
        "outputId": "e89e9de0-0f02-4f31-9791-7c6297aab7cb"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy 0.48398576512455516\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "      moderate       0.57      0.69      0.62      2306\n",
            "not depression       0.71      0.22      0.33      1830\n",
            "        severe       0.16      0.51      0.25       360\n",
            "\n",
            "      accuracy                           0.48      4496\n",
            "     macro avg       0.48      0.47      0.40      4496\n",
            "  weighted avg       0.59      0.48      0.48      4496\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AdaBoost"
      ],
      "metadata": {
        "id": "cWK3dbEM45Fj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# res_x2, res_y2 = sm.fit_resample(train_x_tf, train_y)\n",
        "ada = AdaBoostClassifier()\n",
        "\n",
        "boost = ada.fit(train_x_tf, train_y)\n",
        "y_pred = boost.predict(test_x_tf)\n",
        "print(\"AdaBoost Classifier Model Accuracy:\", accuracy_score(test_y, y_pred))\n",
        "print(metrics.classification_report(test_y, y_pred))"
      ],
      "metadata": {
        "id": "6QDGISapwPVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression"
      ],
      "metadata": {
        "id": "Kz3D-3YoDine"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
        "cv = RepeatedStratifiedKFold(n_splits = 10, n_repeats=3)\n",
        "model = model.fit(res_tfx,res_tfy)\n",
        "y_pred = model.predict(test_x_tf)\n",
        "print(\"Regression Classifier Model Accuracy:\", accuracy_score(test_y, y_pred))\n",
        "print(metrics.classification_report(test_y, y_pred))"
      ],
      "metadata": {
        "id": "hKYzd_kPDmDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word2Vec"
      ],
      "metadata": {
        "id": "5_goD7gM5Bnj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "5998\n",
            "[('still', 0.9996699094772339), ('apparently', 0.9996106624603271), ('come', 0.9996073842048645), ('sound', 0.9995858669281006), ('brother', 0.9995836615562439), ('problem', 0.9995808601379395), ('also', 0.9995766878128052), ('everybody', 0.9995734691619873), ('cause', 0.999566912651062), ('stress', 0.9995615482330322)]\n"
          ]
        }
      ],
      "source": [
        "# Word2Vec reprezentation\n",
        "import gensim \n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.text import one_hot,Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "nltk.download('punkt')\n",
        "corpus_text = 'n'.join(train_x)\n",
        "# corpus_text = 'n'.join(res_x)\n",
        "data = []\n",
        "# iterate through each sentence in the file\n",
        "for i in sent_tokenize(corpus_text):\n",
        "    temp = []\n",
        "    # tokenize the sentence into words\n",
        "    for j in word_tokenize(i):\n",
        "        temp.append(j.lower())\n",
        "    data.append(temp)\n",
        "\n",
        "model1 = gensim.models.Word2Vec(min_count = 5,size = 300, window = 5, alpha = 0.025)\n",
        "model1.build_vocab(data)\n",
        "print(len(model1.wv.vocab))\n",
        "model1.train(data, total_examples = model1.corpus_count, epochs=30)\n",
        "\n",
        "print(model1.wv.most_similar(positive=[\"school\"]))\n",
        "\n",
        "vocab = list(model1.wv.vocab.keys())\n",
        "\n",
        "word2vec_dict ={}\n",
        "for word in vocab:\n",
        "  word2vec_dict[word] = model1.wv.get_vector(word)\n",
        "\n",
        "maxi=-1\n",
        "for i,rev in enumerate(train_x):\n",
        "  tokens=rev.split()\n",
        "  if(len(tokens)>maxi):\n",
        "    maxi=len(tokens)\n",
        "# print(maxi)\n",
        "\n",
        "\n",
        "\n",
        "tok = Tokenizer()\n",
        "tok.fit_on_texts(train_x)\n",
        "vocab_size = len(tok.word_index) + 1\n",
        "encd_rev = tok.texts_to_sequences(train_x)\n",
        "\n",
        "max_len = 1464\n",
        "embed_dim = 300\n",
        "pad_rev = pad_sequences(encd_rev, maxlen=max_len, padding='post')\n",
        "\n",
        "# print(pad_rev.shape)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "iTATudKbQUFe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60ce886d-bb7e-403d-b51b-1be8ddca56c4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network"
      ],
      "metadata": {
        "id": "tUiQnNNV5Frg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.initializers import Constant\n",
        "from keras import layers\n",
        "from keras.layers import ReLU\n",
        "from keras.layers import Dropout\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense , Flatten ,Embedding,Input,CuDNNLSTM,LSTM\n",
        "from keras.models import Model\n",
        "embed_matrix = np.zeros(shape=(vocab_size, embed_dim))\n",
        "for word,i in tok.word_index.items():\n",
        "  embed_vector=word2vec_dict.get(word)\n",
        "  if embed_vector is not None:  # word is in the vocabulary learned by the w2v model\n",
        "    embed_matrix[i]=embed_vector\n",
        "train = []\n",
        "for row in train_y:\n",
        "  if row == \"moderate\":\n",
        "    train.append(1)\n",
        "  elif row == \"severe\":\n",
        "    train.append(2)\n",
        "  elif row == \"not depression\":\n",
        "    train.append(0)\n",
        "\n",
        "Y=keras.utils.to_categorical(train)  # one hot target as required by NN.\n",
        "x_train,x_test,y_train,y_test=train_test_split(pad_rev,Y,test_size=0.25,random_state=42)\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size,output_dim=embed_dim,input_length=max_len,embeddings_initializer=Constant(embed_matrix)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(16,activation='relu'))\n",
        "model.add(Dropout(0.50))\n",
        "# model.add(Dense(16,activation='relu'))\n",
        "# model.add(Dropout(0.20))\n",
        "model.add(Dense(3,activation='softmax'))\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
        "loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model=model.fit(x_train, y_train, epochs=5, batch_size=64, validation_data=(x_test,y_test))\n",
        "# pred_y = model.predict(x_test)\n",
        "# metrics.classification_report(y_test, pred_y)"
      ],
      "metadata": {
        "id": "VyToE_DG7ds6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "c22599b8-8aee-4f91-d122-021954366f17"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "105/105 [==============================] - 37s 337ms/step - loss: 1.5945 - accuracy: 0.6465 - val_loss: 1.1089 - val_accuracy: 0.6802\n",
            "Epoch 2/5\n",
            "105/105 [==============================] - 29s 274ms/step - loss: 1.1826 - accuracy: 0.6758 - val_loss: 0.9744 - val_accuracy: 0.6806\n",
            "Epoch 3/5\n",
            "105/105 [==============================] - 28s 271ms/step - loss: 1.0022 - accuracy: 0.6746 - val_loss: 1.0374 - val_accuracy: 0.6181\n",
            "Epoch 4/5\n",
            "105/105 [==============================] - 29s 273ms/step - loss: 0.9945 - accuracy: 0.6749 - val_loss: 0.8957 - val_accuracy: 0.6806\n",
            "Epoch 5/5\n",
            "105/105 [==============================] - 29s 273ms/step - loss: 0.9948 - accuracy: 0.6753 - val_loss: 0.8679 - val_accuracy: 0.6815\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-58791cc30132>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mpred_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'History' object has no attribute 'predict'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "code.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}