{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanduerhan/Licenta/blob/main/code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OH7f6H-lQfOA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "ZX3Ibl5BQUFC"
      },
      "outputs": [],
      "source": [
        "#importing the dataset\n",
        "import pandas as pd\n",
        "col_list = [\"Column2\", \"Column3\"]\n",
        "dataset = pd.read_excel(\"Training.data.xlsx\", usecols=col_list)\n",
        "testset = pd.read_excel(\"Dev.xlsx\", usecols=col_list)\n",
        "train_x = dataset[\"Column2\"]\n",
        "train_y = dataset[\"Column3\"]\n",
        "test_x = testset[\"Column2\"]\n",
        "test_y = testset[\"Column3\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "# cleaning the text : deleting the emojis, the existing emails, the punctuation, the present digits,\n",
        "# the hyperlinks and the stopwords (a,the,is, etc)\n",
        "import nltk\n",
        "import openpyxl\n",
        "from nltk.corpus import stopwords, words, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re, string\n",
        "from string import punctuation, digits\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "def delete_emoji(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               u\"\\U0001f926-\\U0001f937\"\n",
        "                               u\"\\U00010000-\\U0010ffff\"\n",
        "                               u\"\\u2640-\\u2642\"\n",
        "                               u\"\\u2600-\\u2B55\"\n",
        "                               u\"\\u200d\"\n",
        "                               u\"\\u23cf\"\n",
        "                               u\"\\u23e9\"\n",
        "                               u\"\\u231a\"\n",
        "                               u\"\\ufe0f\"  # dingbats\n",
        "                               u\"\\u3030\"\n",
        "                               \"]+\", re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)  # no emoji\n",
        "\n",
        "\n",
        "def delete_digits(text):\n",
        "    text = text.lower()\n",
        "    clean = text.translate(str.maketrans('', '', digits))\n",
        "    return clean\n",
        "\n",
        "\n",
        "def delete_punctuation(text):\n",
        "    clean = text.translate(str.maketrans('', '', punctuation + '’“”'))\n",
        "    return clean\n",
        "\n",
        "\n",
        "stop = set(stopwords.words('english'))\n",
        "punct = list(string.punctuation)\n",
        "stop.update(punct)\n",
        "\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    final_text = []\n",
        "    for i in text.split():\n",
        "        if i.strip().lower() not in stop:\n",
        "            final_text.append(i.strip())\n",
        "    return \" \".join(final_text)\n",
        "\n",
        "\n",
        "def remove_hyperlinks(text):\n",
        "    for word in text:\n",
        "        if re.match(r'^http', word):\n",
        "            text.remove(word)\n",
        "    return \" \".join(text)\n",
        "\n",
        "\n",
        "def remove_emails(text):\n",
        "    text = text.split()\n",
        "    for i in text:\n",
        "        if '@' in i.strip().lower():\n",
        "            text.remove(i)\n",
        "    return \" \".join(text)\n",
        "\n",
        "\n",
        "def denoise_text(text):\n",
        "    text = remove_emails(text)\n",
        "    text = delete_punctuation(text)\n",
        "    text = delete_emoji(text)\n",
        "    text = delete_digits(text)\n",
        "    text = remove_stopwords(text)\n",
        "    text = text.split()\n",
        "    text = remove_hyperlinks(text)\n",
        "    return text\n",
        "\n",
        "\n",
        "train_x = train_x.apply(denoise_text)\n",
        "test_x = test_x.apply(denoise_text)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fySiVht3QUFH",
        "outputId": "67f7fad5-8942-422e-ca25-3e27837d273c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ],
      "source": [
        "# lemmatization : crying -> cry, days -> day\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "def get_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "\n",
        "def lemm(text):\n",
        "    text = text.split()\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    sar_list_lemmatizer = [lemmatizer.lemmatize(word, get_pos(word)) for word in text]\n",
        "    return \" \".join(sar_list_lemmatizer)\n",
        "\n",
        "train_x = train_x.apply(lemm)\n",
        "test_x = test_x.apply(lemm)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oSwfDyzQUFb",
        "outputId": "e81197ad-c805-4c4a-be38-c242b0550288"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy 0.5293594306049823\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "      moderate       0.56      0.79      0.66      2306\n",
            "not depression       0.68      0.22      0.33      1830\n",
            "        severe       0.24      0.44      0.31       360\n",
            "\n",
            "      accuracy                           0.53      4496\n",
            "     macro avg       0.49      0.48      0.43      4496\n",
            "  weighted avg       0.58      0.53      0.49      4496\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# CountVectorizer reprezentation for the user tweets with Naive Bayes\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "vectorizer = CountVectorizer()\n",
        "x_train_cv = vectorizer.fit_transform(train_x)\n",
        "\n",
        "x_test_cv = vectorizer.transform(test_x)\n",
        "\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "8blt8KSfQUFd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "180a3206-6f81-42a8-e1a0-7f984efdce2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "sm = SMOTE(random_state = 42)\n",
        "\n",
        "\n",
        "\n",
        "res_x, res_y = sm.fit_resample(x_train_cv, train_y)\n",
        "\n",
        "# print(\"After OverSampling, counts of label '1': {}\".format(sum(res_y == \"severe\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-x13nL-a0EM",
        "outputId": "8066a06d-31aa-4112-a6d6-fdcae9e2a957"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After OverSampling, counts of label '1': 6019\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "naive_bayes_classifier = MultinomialNB()\n",
        "naive_bayes_classifier.fit(res_x, res_y)\n",
        "pred_y = naive_bayes_classifier.predict(x_test_cv)\n",
        "\n",
        "score1 = metrics.accuracy_score(test_y, pred_y)\n",
        "print(\"Accuracy \" + str(score1))\n",
        "print(metrics.classification_report(test_y, pred_y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDe2Ts7ZgnZs",
        "outputId": "7b9b2080-fe90-4a3b-d39a-61d6273d0eb5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy 0.5209074733096085\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "      moderate       0.56      0.78      0.65      2306\n",
            "not depression       0.70      0.24      0.35      1830\n",
            "        severe       0.16      0.31      0.21       360\n",
            "\n",
            "      accuracy                           0.52      4496\n",
            "     macro avg       0.48      0.44      0.41      4496\n",
            "  weighted avg       0.59      0.52      0.50      4496\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tf-idf reprezentation for the tweets\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_df = 0.5)\n",
        "train_x_tf = vectorizer.fit_transform(train_x)\n",
        "test_x_tf = vectorizer.transform(test_x)\n",
        "\n",
        "res_x2, res_y2 = sm.fit_resample(train_x_tf, train_y)\n",
        "\n",
        "naive_bayes_classifier = MultinomialNB()\n",
        "naive_bayes_classifier.fit(res_x2, res_y2)\n",
        "pred_y = naive_bayes_classifier.predict(test_x_tf)\n",
        "score1 = metrics.accuracy_score(test_y, pred_y)\n",
        "print(\"Accuracy \" + str(score1))\n",
        "print(metrics.classification_report(test_y, pred_y))\n"
      ],
      "metadata": {
        "id": "WkoI7tMeRnXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_df = 0.5)\n",
        "train_x_tf = vectorizer.fit_transform(train_x)\n",
        "test_x_tf = vectorizer.transform(test_x)\n",
        "# res_x2, res_y2 = sm.fit_resample(train_x_tf, train_y)\n",
        "ada = AdaBoostClassifier()\n",
        "\n",
        "boost = ada.fit(train_x_tf, train_y)\n",
        "y_pred = boost.predict(test_x_tf)\n",
        "print(\"AdaBoost Classifier Model Accuracy:\", accuracy_score(test_y, y_pred))\n",
        "print(metrics.classification_report(test_y, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QDGISapwPVO",
        "outputId": "158715c5-43ab-4e5e-8a81-084ef53fedbc"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Classifier Model Accuracy: 0.5729537366548043\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "      moderate       0.57      0.81      0.67      2306\n",
            "not depression       0.66      0.33      0.44      1830\n",
            "        severe       0.32      0.31      0.32       360\n",
            "\n",
            "      accuracy                           0.57      4496\n",
            "     macro avg       0.52      0.48      0.48      4496\n",
            "  weighted avg       0.59      0.57      0.55      4496\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(res_x)"
      ],
      "metadata": {
        "id": "XIZ2Wzzj2fJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "5998\n",
            "[('still', 0.9996541738510132), ('come', 0.9996047616004944), ('dad', 0.9995818138122559), ('cause', 0.9995653629302979), ('apparently', 0.9995646476745605), ('also', 0.9995605945587158), ('stress', 0.9995446801185608), ('always', 0.9995442628860474), ('best', 0.9995378851890564), ('sound', 0.9995367527008057)]\n",
            "(8891, 1464)\n"
          ]
        }
      ],
      "source": [
        "# Word2Vec reprezentation\n",
        "import gensim \n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.text import one_hot,Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "nltk.download('punkt')\n",
        "corpus_text = 'n'.join(train_x)\n",
        "# corpus_text = 'n'.join(res_x)\n",
        "data = []\n",
        "# iterate through each sentence in the file\n",
        "for i in sent_tokenize(corpus_text):\n",
        "    temp = []\n",
        "    # tokenize the sentence into words\n",
        "    for j in word_tokenize(i):\n",
        "        temp.append(j.lower())\n",
        "    data.append(temp)\n",
        "\n",
        "model1 = gensim.models.Word2Vec(min_count = 5,size = 300, window = 5, alpha = 0.025)\n",
        "model1.build_vocab(data)\n",
        "print(len(model1.wv.vocab))\n",
        "model1.train(data, total_examples = model1.corpus_count, epochs=30)\n",
        "\n",
        "print(model1.wv.most_similar(positive=[\"school\"]))\n",
        "\n",
        "vocab = list(model1.wv.vocab.keys())\n",
        "\n",
        "word2vec_dict ={}\n",
        "for word in vocab:\n",
        "  word2vec_dict[word] = model1.wv.get_vector(word)\n",
        "\n",
        "maxi=-1\n",
        "for i,rev in enumerate(train_x):\n",
        "  tokens=rev.split()\n",
        "  if(len(tokens)>maxi):\n",
        "    maxi=len(tokens)\n",
        "# print(maxi)\n",
        "\n",
        "\n",
        "\n",
        "tok = Tokenizer()\n",
        "tok.fit_on_texts(train_x)\n",
        "vocab_size = len(tok.word_index) + 1\n",
        "encd_rev = tok.texts_to_sequences(train_x)\n",
        "\n",
        "max_len = 1464\n",
        "embed_dim = 300\n",
        "pad_rev = pad_sequences(encd_rev, maxlen=max_len, padding='post')\n",
        "\n",
        "print(pad_rev.shape)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "iTATudKbQUFe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b407cce4-e719-40b9-df57-8e0b3c819c98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.initializers import Constant\n",
        "from keras import layers\n",
        "from keras.layers import ReLU\n",
        "from keras.layers import Dropout\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense , Flatten ,Embedding,Input,CuDNNLSTM,LSTM\n",
        "from keras.models import Model\n",
        "embed_matrix = np.zeros(shape=(vocab_size, embed_dim))\n",
        "for word,i in tok.word_index.items():\n",
        "  embed_vector=word2vec_dict.get(word)\n",
        "  if embed_vector is not None:  # word is in the vocabulary learned by the w2v model\n",
        "    embed_matrix[i]=embed_vector\n",
        "train = []\n",
        "for row in train_y:\n",
        "  if row == \"moderate\":\n",
        "    train.append(1)\n",
        "  elif row == \"severe\":\n",
        "    train.append(2)\n",
        "  elif row == \"not depression\":\n",
        "    train.append(0)\n",
        "\n",
        "Y=keras.utils.to_categorical(train)  # one hot target as required by NN.\n",
        "x_train,x_test,y_train,y_test=train_test_split(pad_rev,Y,test_size=0.20,random_state=42)\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size,output_dim=embed_dim,input_length=max_len,embeddings_initializer=Constant(embed_matrix)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(16,activation='relu'))\n",
        "model.add(Dropout(0.50))\n",
        "# model.add(Dense(16,activation='relu'))\n",
        "# model.add(Dropout(0.20))\n",
        "model.add(Dense(3,activation='softmax'))  # sigmod for bin. classification.\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.RMSprop(lr=1e-3),\n",
        "loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=64, validation_data=(x_test,y_test))"
      ],
      "metadata": {
        "id": "VyToE_DG7ds6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6b66312-9a9a-4022-e7f0-ffeb296fcf6a"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "112/112 [==============================] - 26s 229ms/step - loss: 0.9768 - accuracy: 0.6600 - val_loss: 1.4541 - val_accuracy: 0.5273\n",
            "Epoch 2/5\n",
            "112/112 [==============================] - 26s 229ms/step - loss: 0.8454 - accuracy: 0.6888 - val_loss: 0.7225 - val_accuracy: 0.6897\n",
            "Epoch 3/5\n",
            "112/112 [==============================] - 26s 229ms/step - loss: 0.7705 - accuracy: 0.7009 - val_loss: 0.6593 - val_accuracy: 0.7116\n",
            "Epoch 4/5\n",
            "112/112 [==============================] - 26s 229ms/step - loss: 0.6730 - accuracy: 0.7074 - val_loss: 0.6034 - val_accuracy: 0.7251\n",
            "Epoch 5/5\n",
            "112/112 [==============================] - 26s 229ms/step - loss: 0.6129 - accuracy: 0.7212 - val_loss: 0.5498 - val_accuracy: 0.7212\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4d60d8ea10>"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "code.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}