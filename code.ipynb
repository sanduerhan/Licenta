{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanduerhan/Licenta/blob/main/code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OH7f6H-lQfOA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "ZX3Ibl5BQUFC"
      },
      "outputs": [],
      "source": [
        "#importing the dataset\n",
        "import pandas as pd\n",
        "col_list = [\"Column2\", \"Column3\"]\n",
        "col_list2 = [\"Column2\"]\n",
        "dataset = pd.read_excel(\"Training.data.xlsx\", usecols=col_list)\n",
        "testset = pd.read_excel(\"Test.data.xlsx\", usecols=col_list2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "waiting mind breakdown new year feeling isnt anymore dont know anyone else im little bit worried ill go back depressed days time something last year tried breakdowns start mere days later broke crying wasnt entire year december ok month wait weird way act feel feels bit normal\n"
          ]
        }
      ],
      "source": [
        "# cleaning the text : deleting the emojis, the existing emails, the punctuation, the present digits,\n",
        "# the hyperlinks and the stopwords (a,the,is, etc)\n",
        "import nltk\n",
        "import openpyxl\n",
        "from nltk.corpus import stopwords, words, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re, string\n",
        "from string import punctuation, digits\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "def delete_emoji(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               u\"\\U0001f926-\\U0001f937\"\n",
        "                               u\"\\U00010000-\\U0010ffff\"\n",
        "                               u\"\\u2640-\\u2642\"\n",
        "                               u\"\\u2600-\\u2B55\"\n",
        "                               u\"\\u200d\"\n",
        "                               u\"\\u23cf\"\n",
        "                               u\"\\u23e9\"\n",
        "                               u\"\\u231a\"\n",
        "                               u\"\\ufe0f\"  # dingbats\n",
        "                               u\"\\u3030\"\n",
        "                               \"]+\", re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)  # no emoji\n",
        "\n",
        "\n",
        "def delete_digits(text):\n",
        "    text = text.lower()\n",
        "    clean = text.translate(str.maketrans('', '', digits))\n",
        "    return clean\n",
        "\n",
        "\n",
        "def delete_punctuation(text):\n",
        "    clean = text.translate(str.maketrans('', '', punctuation + '’“”'))\n",
        "    return clean\n",
        "\n",
        "\n",
        "stop = set(stopwords.words('english'))\n",
        "punct = list(string.punctuation)\n",
        "stop.update(punct)\n",
        "\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    final_text = []\n",
        "    for i in text.split():\n",
        "        if i.strip().lower() not in stop:\n",
        "            final_text.append(i.strip())\n",
        "    return \" \".join(final_text)\n",
        "\n",
        "\n",
        "def remove_hyperlinks(text):\n",
        "    for word in text:\n",
        "        if re.match(r'^http', word):\n",
        "            text.remove(word)\n",
        "    return \" \".join(text)\n",
        "\n",
        "\n",
        "def remove_emails(text):\n",
        "    text = text.split()\n",
        "    for i in text:\n",
        "        if '@' in i.strip().lower():\n",
        "            text.remove(i)\n",
        "    return \" \".join(text)\n",
        "\n",
        "\n",
        "def denoise_text(text):\n",
        "    text = remove_emails(text)\n",
        "    text = delete_punctuation(text)\n",
        "    text = delete_emoji(text)\n",
        "    text = delete_digits(text)\n",
        "    text = remove_stopwords(text)\n",
        "    text = text.split()\n",
        "    text = remove_hyperlinks(text)\n",
        "    return text\n",
        "\n",
        "\n",
        "dataset['Column2'] = dataset['Column2'].apply(denoise_text)\n",
        "print(dataset['Column2'][0])"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fySiVht3QUFH",
        "outputId": "52a5c11f-4b7f-49c5-b280-f1c09fe9e2dd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "wait mind breakdown new year feel isnt anymore dont know anyone else im little bit worried ill go back depressed day time something last year try breakdown start mere day later broke cry wasnt entire year december ok month wait weird way act feel feel bit normal\n"
          ]
        }
      ],
      "source": [
        "# lemmatization : crying -> cry, days -> day\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "def get_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "\n",
        "def lemm(text):\n",
        "    text = text.split()\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    sar_list_lemmatizer = [lemmatizer.lemmatize(word, get_pos(word)) for word in text]\n",
        "    return \" \".join(sar_list_lemmatizer)\n",
        "\n",
        "print(lemm(dataset['Column2'][0]))"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oSwfDyzQUFb",
        "outputId": "3ea59973-858a-4c53-d004-2c49fc813ba2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# CountVectorizer reprezentation for the user tweets\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "x_train = dataset['Column2']\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(x_train)\n",
        "x_train_dtm = vectorizer.transform(x_train)\n",
        "\n",
        "x_train_dtm = vectorizer.fit_transform(x_train)\n",
        "print(x_train_dtm)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "8blt8KSfQUFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tf-idf reprezentation for the tweets\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "response = vectorizer.fit_transform(dataset['Column2'])\n",
        "print(response)"
      ],
      "metadata": {
        "id": "WkoI7tMeRnXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Word2Vec reprezentation\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "iTATudKbQUFe"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "code.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}