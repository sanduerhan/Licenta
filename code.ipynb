{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanduerhan/Licenta/blob/main/code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OH7f6H-lQfOA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Import"
      ],
      "metadata": {
        "id": "EOmssdxm3o5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "RI7LFohhh6oy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43069b54-333d-4b1b-fadd-27f785df788b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHx29AkXpS9C",
        "outputId": "707bfaa3-9537-408f-fd9a-0f2ea6d38235"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "id": "ZX3Ibl5BQUFC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import openpyxl\n",
        "from nltk.corpus import stopwords, words, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "import gensim\n",
        "import re, string\n",
        "from string import punctuation, digits\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import one_hot,Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.initializers import Constant\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import ReLU\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense , Flatten ,Embedding,Input,LSTM\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "col_list = [\"Column2\", \"Column3\"]\n",
        "dataset = pd.read_excel(\"Training.data.xlsx\", usecols=col_list)\n",
        "testset = pd.read_excel(\"Dev.xlsx\", usecols=col_list)\n",
        "\n",
        "\n",
        "def add_category_id(dataset):\n",
        "  dataset['category_id'] = dataset['Column3'].factorize()[0]\n",
        "  category_id_dataset = dataset[['Column3', 'category_id']].drop_duplicates()\n",
        "\n",
        "  category_to_id = dict(category_id_dataset.values)\n",
        "  id_to_category = dict(category_id_dataset[['category_id', 'Column3']].values)\n",
        "  return dataset,category_to_id, id_to_category\n",
        "\n",
        "dataset, category_to_id_train, id_to_category_train = add_category_id(dataset)\n",
        "testset, category_to_id_test, id_to_category_test = add_category_id(testset)\n",
        "dataset = dataset.sample(frac = 1)\n",
        "testset = testset.sample(frac = 1)\n",
        "train_x = dataset[\"Column2\"]\n",
        "train_y = dataset[\"category_id\"]\n",
        "test_x = testset[\"Column2\"]\n",
        "test_y = testset[\"category_id\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset[\"Column2\"].head)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yy4EFKrVeZaq",
        "outputId": "f5bc8132-6fec-43e0-a3dd-5601d4a0d3a2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method NDFrame.head of 6237    Thank you all : This sub is the only place whe...\n",
            "6831    Do this when it gets too hard to handle : Ther...\n",
            "6585    Youre all tough. : Youre all strong people, yo...\n",
            "6769    Randomly Feel like dying : You can have a dece...\n",
            "4241    I feel like my NYE kiss was from a dementor. :...\n",
            "                              ...                        \n",
            "3763    How to stop leaning on people when you are hav...\n",
            "1386    to everyone, : happy new year. making my first...\n",
            "7262    Don’t fucking comment : This is my note, fuck ...\n",
            "441     Should I end the year with a literal fucking b...\n",
            "1733    Plushcare? : Has anyone used Plushcare to get ...\n",
            "Name: Column2, Length: 8068, dtype: object>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_tweets = dataset['Column2'].tolist()\n",
        "test_tweets = testset['Column2'].tolist()\n",
        "def keep_uniques(array, df):\n",
        "    dels=[]\n",
        "    for i in array:\n",
        "        if array.count(i)>1:\n",
        "            dels.append(i)\n",
        "    dels=list(set(dels))\n",
        "    for i in dels:\n",
        "        df.drop( df[ df['Column2'] == i ].index, inplace=True)\n",
        "    return df\n",
        "\n",
        "dataset = keep_uniques(train_tweets, dataset)\n",
        "testset = keep_uniques(test_tweets, testset)\n",
        "\n",
        "print(len(testset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_jdBb8Z_Pus",
        "outputId": "4bdd86af-ceb3-45a2-bd83-5498c1b58d84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(8,6))\n",
        "colors = ['darkblue','darkblue','darkblue']\n",
        "dataset.groupby('Column3').Column2.count().sort_values().plot.barh(\n",
        "    ylim=0, color=colors, title= 'NUMBER OF TWEETS IN EACH CATEGORY\\n')\n",
        "plt.xlabel('Number of ocurrences', fontsize = 10);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "PQ4_yB2Cdnad",
        "outputId": "4bd4d6b4-c125-42bc-941a-1adcbdf23365"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PreProcessing"
      ],
      "metadata": {
        "id": "xHx-Yr8O3pvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tweet-preprocessor\n",
        "import preprocessor as p\n",
        "\n",
        "def preprocess_tweets(text):\n",
        "  return p.clean(text)\n",
        "\n",
        "print(preprocess_tweets(train_x[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5ORHpDKp2eO",
        "outputId": "c5467aca-525a-4762-f9da-92b154e9ce36"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tweet-preprocessor\n",
            "  Downloading tweet_preprocessor-0.6.0-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: tweet-preprocessor\n",
            "Successfully installed tweet-preprocessor-0.6.0\n",
            "Waiting for my mind to have a breakdown once the New Year feeling isnt there anymore : I dont know about anyone else, but Im a little bit worried that Ill go back to being depressed in a few days time or something. Last year, I tried not to have any breakdowns for the start of . A mere days later, I broke down crying. I wasnt the same for that entire year. Up until December, where I was ok that month. Now I just wait... its a weird way to act and feel, but it feels a bit normal.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# cleaning the text : deleting the emojis, the existing emails, the punctuation, the present digits,\n",
        "# the hyperlinks and the stopwords (a,the,is, etc)\n",
        "\n",
        "def delete_emoji(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               u\"\\U0001f926-\\U0001f937\"\n",
        "                               u\"\\U00010000-\\U0010ffff\"\n",
        "                               u\"\\u2640-\\u2642\"\n",
        "                               u\"\\u2600-\\u2B55\"\n",
        "                               u\"\\u200d\"\n",
        "                               u\"\\u23cf\"\n",
        "                               u\"\\u23e9\"\n",
        "                               u\"\\u231a\"\n",
        "                               u\"\\ufe0f\"  # dingbats\n",
        "                               u\"\\u3030\"\n",
        "                               \"]+\", re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)  # no emoji\n",
        "\n",
        "\n",
        "def delete_digits(text):\n",
        "    text = text.lower()\n",
        "    clean = text.translate(str.maketrans('', '', digits))\n",
        "    return clean\n",
        "\n",
        "\n",
        "def delete_punctuation(text):\n",
        "    clean = text.translate(str.maketrans('', '', punctuation + '’“”'))\n",
        "    return clean\n",
        "\n",
        "\n",
        "stop = set(stopwords.words('english'))\n",
        "stop.add('gtpoplt')\n",
        "stop.add('new')\n",
        "stop.add('year')\n",
        "stop.add('eve')\n",
        "stop.add('years')\n",
        "stop.add('ti')\n",
        "stop.add('ame')\n",
        "stop.add('folks')\n",
        "punct = list(string.punctuation)\n",
        "stop.update(punct)\n",
        "\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    final_text = []\n",
        "    for i in text.split():\n",
        "        if i.strip().lower() not in stop:\n",
        "            final_text.append(i.strip())\n",
        "    return \" \".join(final_text)\n",
        "\n",
        "\n",
        "def remove_hyperlinks(text):\n",
        "    for word in text:\n",
        "        if re.match(r'^http', word):\n",
        "            text.remove(word)\n",
        "    return \" \".join(text)\n",
        "\n",
        "def remove_emails(text):\n",
        "    text = text.split()\n",
        "    for i in text:\n",
        "        if '@' in i.strip().lower():\n",
        "            text.remove(i)\n",
        "    return \" \".join(text)\n",
        "\n",
        "contraction_dict = {\"youre\":\"you are\",\"im\": \"i am\",\"wouldnt\": \"would not\",\"itll\": \"it will\",\"wasnt\": \"was not\",\"dont\": \"do not\",\"ill\": \"i will\",\"isnt\": \"is not\",\"cant\": \"cannot\",\"arent\": \"are not\",\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
        "def _get_contractions(contraction_dict):\n",
        "    contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n",
        "    return contraction_dict, contraction_re\n",
        "contractions, contractions_re = _get_contractions(contraction_dict)\n",
        "def replace_contractions(text):\n",
        "    def replace(match):\n",
        "        return contractions[match.group(0)]\n",
        "    return contractions_re.sub(replace, text)\n",
        "\n",
        "# def denoise_text(text):\n",
        "#     text = replace_contractions(text)\n",
        "#     text = remove_words(text)\n",
        "#     text = remove_emails(text)\n",
        "#     text = delete_punctuation(text)\n",
        "#     text = delete_emoji(text)\n",
        "#     text = delete_digits(text)\n",
        "#     text = remove_stopwords(text)\n",
        "#     text = text.split()\n",
        "#     text = remove_hyperlinks(text)\n",
        "#     return text\n",
        "\n",
        "\n",
        "\n",
        "train_x = dataset[\"Column2\"]\n",
        "train_y = dataset[\"category_id\"]\n",
        "test_x = testset[\"Column2\"]\n",
        "test_y = testset[\"category_id\"]\n",
        "\n",
        "for i in range(len(train_x)):\n",
        "  train_x[i] = replace_contractions(train_x[i])\n",
        "  train_x[i] = remove_emails(train_x[i])\n",
        "  train_x[i] = delete_punctuation(train_x[i])\n",
        "  train_x[i] = delete_emoji(train_x[i])\n",
        "  train_x[i] = delete_digits(train_x[i])\n",
        "  train_x[i] = remove_stopwords(train_x[i])\n",
        "\n",
        "for i in range(len(test_x)):\n",
        "  test_x[i] = replace_contractions(test_x[i])\n",
        "  test_x[i] = remove_emails(test_x[i])\n",
        "  test_x[i] = delete_punctuation(test_x[i])\n",
        "  test_x[i] = delete_emoji(test_x[i])\n",
        "  test_x[i] = delete_digits(test_x[i])\n",
        "  test_x[i] = remove_stopwords(test_x[i])\n",
        "\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "fySiVht3QUFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_x[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfI378zkgRMG",
        "outputId": "99cace62-957a-4892-d6cd-e29b035d602c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "waiting mind breakdown feeling anymore know anyone else little bit worried go back depressed days something last tried breakdowns start mere days later broke crying entire december ok month wait weird way act feel feels bit normal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatization"
      ],
      "metadata": {
        "id": "cfS5G2jQ4GGh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "source": [
        "# lemmatization : crying -> cry, days -> day\n",
        "\n",
        "def get_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "\n",
        "def lemm(text):\n",
        "    text = text.split()\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    sar_list_lemmatizer = [lemmatizer.lemmatize(word, get_pos(word)) for word in text]\n",
        "    return \" \".join(sar_list_lemmatizer)\n",
        "\n",
        "train_x = train_x.apply(lemm)\n",
        "test_x = test_x.apply(lemm)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "3oSwfDyzQUFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_x[0])"
      ],
      "metadata": {
        "id": "0Gst2fP9GIvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "text = ' '.join(train_x[dataset['Column3']=='severe'])\n",
        "wc = WordCloud(background_color='white').generate(text)\n",
        "plt.imshow(wc)"
      ],
      "metadata": {
        "id": "t0Oao20MI9Mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Count Vectorizer"
      ],
      "metadata": {
        "id": "IMHPbMBI4VTQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8068, 10252)\n"
          ]
        }
      ],
      "source": [
        "# CountVectorizer reprezentation for the user tweets\n",
        "vectorizer = CountVectorizer()\n",
        "x_train_cv = vectorizer.fit_transform(train_x)\n",
        "\n",
        "x_test_cv = vectorizer.transform(test_x)\n",
        "print(x_train_cv.shape)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "8blt8KSfQUFd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e82e995-770d-43cf-9733-391e0429529d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data imbalance handling"
      ],
      "metadata": {
        "id": "7MBdfMwk4bR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sm = SMOTE(random_state = 42)\n",
        "res_x, res_y = sm.fit_resample(x_train_cv, train_y)\n",
        "print(\"After OverSampling, counts of label '2': {}\".format(sum(res_y == 2)))"
      ],
      "metadata": {
        "id": "w-x13nL-a0EM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "935333c4-69ee-48cb-9e18-b10b326c365b"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After OverSampling, counts of label '2': 5647\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os = RandomOverSampler()\n",
        "res_x2, res_y2 = os.fit_resample(x_train_cv, train_y)"
      ],
      "metadata": {
        "id": "020ZvbDV8Yk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tf-Idf "
      ],
      "metadata": {
        "id": "2abJM4PN4yyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tf-idf reprezentation for the tweets\n",
        "from sklearn.feature_selection import chi2\n",
        "vectorizer = TfidfVectorizer(max_df = 0.15,min_df = 5, ngram_range=(1,2), stop_words='english')\n",
        "train_x_tf = vectorizer.fit_transform(train_x)\n",
        "test_x_tf = vectorizer.transform(test_x)\n",
        "N = 2\n",
        "# for Product, category_id in sorted(category_to_id_train.items()):\n",
        "#     features_chi2 = chi2(train_x_tf, train_y == category_id)\n",
        "#     indices = np.argsort(features_chi2[0])\n",
        "#     feature_names = np.array(vectorizer.get_feature_names())[indices]\n",
        "#     unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
        "#     bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
        "#     print(\"# '{}':\".format(Product))\n",
        "#     print(\" Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-N:])))\n",
        "#     print(\" Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-N:])))\n",
        "res_tfx, res_tfy = sm.fit_resample(train_x_tf, train_y)\n",
        "print(train_x_tf.shape)\n",
        "print(res_tfx.shape)"
      ],
      "metadata": {
        "id": "WkoI7tMeRnXz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e2e9b9f-567d-464c-a539-9d76f4e02498"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8068, 33909)\n",
            "(16941, 33909)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word2Vec"
      ],
      "metadata": {
        "id": "5_goD7gM5Bnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent = [row.split() for row in train_x]"
      ],
      "metadata": {
        "id": "Q_TNLIZYk8DA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.phrases import Phrases, Phraser\n"
      ],
      "metadata": {
        "id": "wH95fWT2lHE2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram = Phraser(phrases)\n",
        "sentences = bigram[sent]"
      ],
      "metadata": {
        "id": "ffE5uhQflNF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentences)"
      ],
      "metadata": {
        "id": "xslKOh8bnezF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model = Word2Vec(min_count=20,\n",
        "                     window=5,\n",
        "                     size=300,\n",
        "                     sample=6e-5, \n",
        "                     alpha=0.03, \n",
        "                     min_alpha=0.0007, \n",
        "                     negative=20,\n",
        "                     )"
      ],
      "metadata": {
        "id": "XP-cfBanlg4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.build_vocab(sentences)"
      ],
      "metadata": {
        "id": "FjD343pblz0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHmC66qBl6T1",
        "outputId": "9a10147b-62c4-48a5-ef56-28f41040b0a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5675934, 16754460)"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.wv.most_similar(positive=[\"depression\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSI6JFcmmM43",
        "outputId": "06bb1c61-8a23-4543-c452-77acb4935e3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('anxiety', 0.5718693733215332),\n",
              " ('diagnose', 0.5230121612548828),\n",
              " ('disorder', 0.4272606372833252),\n",
              " ('ptsd', 0.3910037577152252),\n",
              " ('severe', 0.3898566663265228),\n",
              " ('adhd', 0.37263554334640503),\n",
              " ('recently', 0.37178850173950195),\n",
              " ('symptom', 0.3715326488018036),\n",
              " ('battling', 0.3693320155143738),\n",
              " ('struggle', 0.35999664664268494)]"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def word2(train_x):\n",
        "  mes = []\n",
        "  for i in train_x:\n",
        "    mes.append(i.split())\n",
        "  word2vec_model = Word2Vec(mes, size=300, window=5, min_count=1)\n",
        "  token = Tokenizer()\n",
        "  token.fit_on_texts(mes)\n",
        "  text = token.texts_to_sequences(mes)\n",
        "  text = pad_sequences(text, 1464)\n",
        "  word_index = token.word_index\n",
        "  return word2vec_model,word_index,text"
      ],
      "metadata": {
        "id": "eSgyTFdxu0dX"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model,word_index,pad_rev_train = word2(train_x)"
      ],
      "metadata": {
        "id": "eJRUq5f6v1FO"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2,word_index2,pad_rev_test = word2(test_x)"
      ],
      "metadata": {
        "id": "kwKmyoLL0HF6"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.wv.most_similar(\"sex\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoP4ZQ7kv7HW",
        "outputId": "1d7e6aa1-83ff-4df0-e7b2-e8260113812d"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('available', 0.9989280700683594),\n",
              " ('circle', 0.9982731342315674),\n",
              " ('confess', 0.9981011152267456),\n",
              " ('woman', 0.9979820847511292),\n",
              " ('business', 0.9978442192077637),\n",
              " ('host', 0.9977796673774719),\n",
              " ('bond', 0.997644305229187),\n",
              " ('contact', 0.9974455237388611),\n",
              " ('argue', 0.997405469417572),\n",
              " ('manipulate', 0.9973846673965454)]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_weight_matrix(model, vocab):\n",
        "    # total vocabulary size plus 0 for unknown words\n",
        "    vocab_size = len(vocab) + 1\n",
        "    # define weight matrix dimensions with all 0\n",
        "    weight_matrix = np.zeros((vocab_size, 300))\n",
        "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
        "    for word, i in vocab.items():\n",
        "        weight_matrix[i] = model[word]\n",
        "    return weight_matrix"
      ],
      "metadata": {
        "id": "O25ga7HBxIk5"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_vectors = get_weight_matrix(model, word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBosyRN8xMkY",
        "outputId": "3f3fce21-2afe-4a79-aa28-5024f4947cfd"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def w2v_embedding(train_text):\n",
        "  ## create list of lists of unigrams\n",
        "  lst_corpus = []\n",
        "  for string in train_text:\n",
        "    lst_words = string.split()\n",
        "    lst_grams = [\" \".join(lst_words[i:i+1]) \n",
        "                for i in range(0, len(lst_words), 1)]\n",
        "    lst_corpus.append(lst_grams)\n",
        "\n",
        "  ## detect bigrams and trigrams\n",
        "  bigrams_detector = gensim.models.phrases.Phrases(lst_corpus, \n",
        "                  delimiter=\" \".encode(), min_count=30)\n",
        "  bigrams_detector = gensim.models.phrases.Phraser(bigrams_detector)\n",
        "  trigrams_detector = gensim.models.phrases.Phrases(bigrams_detector[lst_corpus], \n",
        "              delimiter=\" \".encode(), min_count=30)\n",
        "  trigrams_detector = gensim.models.phrases.Phraser(trigrams_detector)\n",
        "\n",
        "  nlp = Word2Vec(lst_corpus, size=300,   \n",
        "            window=8, min_count=2, sg=1, iter=30)\n",
        "  return nlp, lst_corpus, bigrams_detector, trigrams_detector"
      ],
      "metadata": {
        "id": "YqW8Yiqr4vEL"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_engineering(lst_corpus, train_text):\n",
        "  tokenizer = Tokenizer(lower=True, split=' ', \n",
        "                     oov_token=\"NaN\", \n",
        "                     filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
        "  tokenizer.fit_on_texts(lst_corpus)\n",
        "  dic_vocabulary = tokenizer.word_index\n",
        "  ## create sequence\n",
        "  lst_corpus = []\n",
        "  for string in train_text:\n",
        "    lst_words = string.split()\n",
        "    lst_grams = [\" \".join(lst_words[i:i+1]) \n",
        "                for i in range(0, len(lst_words), 1)]\n",
        "    lst_corpus.append(lst_grams)\n",
        "  lst_text2seq= tokenizer.texts_to_sequences(lst_corpus)\n",
        "  ## padding sequence\n",
        "  X_train = pad_sequences(lst_text2seq, \n",
        "                      maxlen=30, padding=\"post\", truncating=\"post\")\n",
        "  return tokenizer, dic_vocabulary, X_train"
      ],
      "metadata": {
        "id": "HXONvgmg40qr"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_embedding_matrix(dic_vocabulary, nlp):\n",
        "  ## start the matrix (length of vocabulary x vector size) with all 0s\n",
        "  embeddings = np.zeros((len(dic_vocabulary)+1, 300))\n",
        "  for word,idx in dic_vocabulary.items():\n",
        "      ## update the row with vector\n",
        "      try:\n",
        "          embeddings[idx] =  nlp[word]\n",
        "      ## if word not in model then skip and the row stays all 0s\n",
        "      except:\n",
        "          pass\n",
        "  return embeddings"
      ],
      "metadata": {
        "id": "w3XdYm2A421r"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp, lst_corpus, bigrams_detector, trigrams_detector = w2v_embedding(train_x)"
      ],
      "metadata": {
        "id": "eYbWGZel4488"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(nlp.wv.most_similar(positive=\"depression\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WIjrfeL9fYT",
        "outputId": "07aa101f-1d5e-4283-fa18-06c1cb12fb27"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('stabilizer', 0.3611605167388916), ('microdosing', 0.34207403659820557), ('anxiety', 0.3333049416542053), ('diagnose', 0.3303808569908142), ('phobia', 0.32765257358551025), ('mercury', 0.325138658285141), ('feverbaby', 0.3229295313358307), ('um', 0.32083916664123535), ('sought', 0.3179461359977722), ('intuniv', 0.3177012801170349)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer, dic_vocabulary, X_train = feature_engineering(lst_corpus,train_x)"
      ],
      "metadata": {
        "id": "4O1vFy469Jaf"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYBf8fPNBzrc",
        "outputId": "16984993-2e68-42e3-cdc1-6b5c6395e7fd"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8068, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_train = make_embedding_matrix(dic_vocabulary, nlp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3JJiPOE9QLZ",
        "outputId": "55e0315b-12e8-4336-ec73-27c33e5711f4"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  import sys\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_handling(test_text,bigrams_detector, trigrams_detector, tokenizer):\n",
        "  ## create list of n-grams\n",
        "  lst_corpus = []\n",
        "  for string in test_text:\n",
        "      lst_words = string.split()\n",
        "      lst_grams = [\" \".join(lst_words[i:i+1]) for i in range(0, \n",
        "                  len(lst_words), 1)]\n",
        "      lst_corpus.append(lst_grams)\n",
        "      \n",
        "  ## detect common bigrams and trigrams using the fitted detectors\n",
        "  lst_corpus = list(bigrams_detector[lst_corpus])\n",
        "  lst_corpus = list(trigrams_detector[lst_corpus])\n",
        "  ## text to sequence with the fitted tokenizer\n",
        "  lst_text2seq = tokenizer.texts_to_sequences(lst_corpus)\n",
        "\n",
        "  ## padding sequence\n",
        "  X_test = pad_sequences(lst_text2seq, maxlen=30,\n",
        "              padding=\"post\", truncating=\"post\")\n",
        "  return X_test"
      ],
      "metadata": {
        "id": "iH5BAwFM_F8Y"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp2, lst_corpus2, bigrams_detector2, trigrams_detector2 = w2v_embedding(test_x)"
      ],
      "metadata": {
        "id": "JGH7zCw3C3mA"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = test_handling(test_x, bigrams_detector2, trigrams_detector2, tokenizer)"
      ],
      "metadata": {
        "id": "P7S9fXX7-E3v"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IH1zz40QCuUi",
        "outputId": "a974d23a-a64f-433d-dc6f-ff15ae4432ee"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4078, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes"
      ],
      "metadata": {
        "id": "KsTCPqZS4jzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "naive_bayes_classifier = MultinomialNB()\n",
        "naive_bayes_classifier.fit(res_x, res_y)\n",
        "pred_y = naive_bayes_classifier.predict(x_test_cv)\n",
        "\n",
        "score1 = metrics.accuracy_score(test_y, pred_y)\n",
        "print(\"Accuracy \" + str(score1))\n",
        "print(metrics.classification_report(test_y, pred_y, digits = 4))"
      ],
      "metadata": {
        "id": "uDe2Ts7ZgnZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "naive_bayes_classifier = MultinomialNB()\n",
        "naive_bayes_classifier.fit(res_tfx, res_tfy)\n",
        "# naive_bayes_classifier.fit(train_x_tf, train_y)\n",
        "pred_y = naive_bayes_classifier.predict(test_x_tf)\n",
        "score1 = metrics.accuracy_score(test_y, pred_y)\n",
        "print(\"Accuracy \" + str(score1))\n",
        "print(metrics.classification_report(test_y, pred_y, digits = 4))"
      ],
      "metadata": {
        "id": "UVkenp6V8Qce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AdaBoost"
      ],
      "metadata": {
        "id": "cWK3dbEM45Fj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# res_x2, res_y2 = sm.fit_resample(train_x_tf, train_y)\n",
        "ada = AdaBoostClassifier()\n",
        "\n",
        "# boost = ada.fit(train_x_tf, train_y)\n",
        "boost = ada.fit(res_tfx, res_tfy)\n",
        "pred_y = boost.predict(test_x_tf)\n",
        "print(\"AdaBoost Classifier Model Accuracy:\", accuracy_score(test_y, pred_y))\n",
        "print(metrics.classification_report(test_y, pred_y))"
      ],
      "metadata": {
        "id": "6QDGISapwPVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression"
      ],
      "metadata": {
        "id": "Kz3D-3YoDine"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_weight = {0: 1.,\n",
        "                1: 3.,\n",
        "                2: 4.}\n",
        "\n",
        "model = LogisticRegression(multi_class='multinomial', solver='newton-cg')\n",
        "# cv = RepeatedStratifiedKFold(n_splits = 10, n_repeats=3)\n",
        "model = model.fit(res_x, res_y)\n",
        "y_pred = model.predict(x_test_cv)\n",
        "\n",
        "# model = model.fit(res_tfx, res_tfy)\n",
        "# y_pred = model.predict(test_x_tf)\n",
        "\n",
        "print(\"Regression Classifier Model Accuracy:\", accuracy_score(test_y, y_pred))\n",
        "print(metrics.classification_report(test_y, y_pred, digits = 4))\n",
        "\n",
        "conf_mat = confusion_matrix(test_y, y_pred)\n",
        "print(conf_mat)\n"
      ],
      "metadata": {
        "id": "hKYzd_kPDmDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class_weight = {0: 1.,\n",
        "                1: 3.,\n",
        "                2: 6.}\n",
        "model = LinearSVC()\n",
        "# model = model.fit(train_x_tf, train_y)\n",
        "# y_pred = model.predict(test_x_tf)\n",
        "model = model.fit(res_x, res_y)\n",
        "y_pred = model.predict(x_test_cv)\n",
        "print(\"Svc Classifier Model Accuracy:\", accuracy_score(test_y, y_pred))\n",
        "print(metrics.classification_report(test_y, y_pred, digits = 4))\n",
        "\n",
        "conf_mat = confusion_matrix(test_y, y_pred)\n",
        "print(conf_mat)\n"
      ],
      "metadata": {
        "id": "wDRy1f9pv-I1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class_weight = {0: 1.,\n",
        "                1: 3.,\n",
        "                2: 5.}\n",
        "model = RandomForestClassifier(class_weight=class_weight)\n",
        "model = model.fit(train_x_tf,train_y)\n",
        "y_pred = model.predict(test_x_tf)\n",
        "print(\"RandomForest Classifier Model Accuracy:\", accuracy_score(test_y, y_pred))\n",
        "print(metrics.classification_report(test_y, y_pred))\n",
        "\n",
        "conf_mat = confusion_matrix(test_y, y_pred)\n",
        "print(conf_mat)"
      ],
      "metadata": {
        "id": "aPLXrd2Cb32y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# F1 metric"
      ],
      "metadata": {
        "id": "lgvLSOR8gsOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def f1(y_true, y_pred):\n",
        "    y_pred = K.round(y_pred)\n",
        "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
        "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
        "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
        "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
        "\n",
        "    p = tp / (tp + fp + K.epsilon())\n",
        "    r = tp / (tp + fn + K.epsilon())\n",
        "\n",
        "    f1 = 2*p*r / (p+r+K.epsilon())\n",
        "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
        "    return K.mean(f1)\n",
        "\n",
        "def f1_loss(y_true, y_pred):\n",
        "    \n",
        "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
        "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
        "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
        "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
        "\n",
        "    p = tp / (tp + fp + K.epsilon())\n",
        "    r = tp / (tp + fn + K.epsilon())\n",
        "\n",
        "    f1 = 2*p*r / (p+r+K.epsilon())\n",
        "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
        "    return 1 - K.mean(f1)"
      ],
      "metadata": {
        "id": "bdQgAL5I6cCW"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network"
      ],
      "metadata": {
        "id": "tUiQnNNV5Frg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_weight = {0: 1.,\n",
        "                1: 3.,\n",
        "                2: 5.}\n",
        "# def split(pad_rev,train_y):\n",
        "#   Y=keras.utils.to_categorical(train_y)  # one hot target as required by NN.\n",
        "#   x_train,x_test,y_train,y_test=train_test_split(pad_rev,Y,test_size=0.05,random_state=42)\n",
        "#   return x_train, x_test,y_train,y_test\n",
        "def network(embeddings):\n",
        " ## input\n",
        "  x_in = layers.Input(shape=(30,))\n",
        "  ## embedding\n",
        "  x = layers.Embedding(input_dim=embeddings.shape[0],  \n",
        "                      output_dim=embeddings.shape[1], \n",
        "                      weights=[embeddings],\n",
        "                      input_length=30, trainable=False)(x_in)\n",
        "  ## 2 layers of bidirectional lstm\n",
        "  x = layers.Bidirectional(layers.LSTM(units=30, dropout=0.2, \n",
        "                          return_sequences=True))(x)\n",
        "  x = layers.Bidirectional(layers.LSTM(units=30, dropout=0.2))(x)\n",
        "  ## final dense layers\n",
        "  x = layers.Dense(64, activation='elu')(x)\n",
        "  y_out = layers.Dense(3, activation='softmax')(x)\n",
        "  ## compile\n",
        "  model = Model(x_in, y_out)\n",
        "  model.compile(loss=f1_loss,\n",
        "                optimizer='adam', metrics=[f1])\n",
        "  return model\n",
        "Y=keras.utils.to_categorical(train_y)\n",
        "Y2=keras.utils.to_categorical(test_y)\n",
        "# x_train,x_test,y_train,y_test = split(X_train,train_y)\n",
        "model = network(embeddings_train)\n",
        "# x_valid, x_x, y_valid, y_y = split(x_test,test_y)\n",
        "model.fit(X_train, Y, epochs=10, batch_size=64, validation_data=(x_test,Y2))\n"
      ],
      "metadata": {
        "id": "QAeWwiGyMJFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_y = model.predict(x_test)\n",
        "pred_y = np.argmax(pred_y, axis=1)\n",
        "# y_test=np.argmax(y_test, axis=1)\n",
        "print(metrics.classification_report(test_y, pred_y, digits = 4))\n",
        "\n",
        "conf_mat = confusion_matrix(test_y, pred_y)\n",
        "print(conf_mat)"
      ],
      "metadata": {
        "id": "tHB9SVT5O4On"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT"
      ],
      "metadata": {
        "id": "fccPGYtiwkXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "hdP6B25p2WM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "OOyweo6YFNFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.initializers import GlorotNormal\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "gr1JKdNUF9yM"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer(\"all-mpnet-base-v2\")"
      ],
      "metadata": {
        "id": "cz2XLEkFgy2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence(texts):\n",
        "  model = SentenceTransformer(\"roberta-large-nli-stsb-mean-tokens\")\n",
        "  embeddings = model.encode(texts)\n",
        "\n",
        "  return embeddings"
      ],
      "metadata": {
        "id": "IBHUesv8FbMC"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRANSFORMER_BATCH=128\n",
        "\n",
        "def count_embedd (df):\n",
        "    idx_chunk=list(df.columns).index('Column2')\n",
        "    embedd_lst = []\n",
        "    for index in range (0, len(df), TRANSFORMER_BATCH):\n",
        "        embedds = model.encode(df.iloc[index:index+TRANSFORMER_BATCH, idx_chunk].values, show_progress_bar=False)\n",
        "        embedd_lst.append(embedds)\n",
        "    return np.concatenate(embedd_lst)"
      ],
      "metadata": {
        "id": "HIZk6hn2aiu_"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_emb = count_embedd(dataset)"
      ],
      "metadata": {
        "id": "jyItDXSMgnog"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_emb = count_embedd(testset)"
      ],
      "metadata": {
        "id": "-4U0N7MSg4Wr"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.array(train_emb)\n",
        "X_test = np.array(test_emb)"
      ],
      "metadata": {
        "id": "CCkIOGTrbp2s"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train=keras.utils.to_categorical(train_y)"
      ],
      "metadata": {
        "id": "tf8ht-tgb_n_"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KERAS_VALIDATION_SPLIT=0.00\n",
        "KERAS_EPOCHS=10\n",
        "KERAS_BATCH_SIZE=16\n",
        "\n",
        "class_weight = {0: 1.,\n",
        "                1: 3.,\n",
        "                2: 6.}\n",
        "\n",
        "# Create and train Keras model\n",
        "n_features=X_train.shape[1]\n",
        "n_labels = y_train.shape[1]\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Dense(2048, input_dim=n_features),\n",
        "  tf.keras.layers.LeakyReLU(alpha=0.01),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(64),\n",
        "  tf.keras.layers.LeakyReLU(alpha=0.01),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "LR=5e-4\n",
        "adam = keras.optimizers.Adam(learning_rate=LR, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "\n",
        "model.compile(optimizer=adam, \n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[f1])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=KERAS_EPOCHS, batch_size=KERAS_BATCH_SIZE, validation_split=KERAS_VALIDATION_SPLIT)"
      ],
      "metadata": {
        "id": "HiKl8FLGb3ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_preds = model.predict(X_test)\n",
        "predicted = np.argmax(y_preds,axis=1) \n",
        "accuracy = metrics.accuracy_score(test_y, predicted)\n",
        "print(\"Accuracy:\",  round(accuracy,4))\n",
        "print(metrics.classification_report(test_y, predicted,digits=4))\n",
        "\n",
        "conf_mat = confusion_matrix(test_y, predicted)\n",
        "print(conf_mat)"
      ],
      "metadata": {
        "id": "vyOx-aVqcLte"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "code.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "IMHPbMBI4VTQ",
        "7MBdfMwk4bR_",
        "2abJM4PN4yyY",
        "5_goD7gM5Bnj",
        "KsTCPqZS4jzh",
        "cWK3dbEM45Fj",
        "Kz3D-3YoDine",
        "lgvLSOR8gsOn",
        "tUiQnNNV5Frg"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}