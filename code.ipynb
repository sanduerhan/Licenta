{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanduerhan/Licenta/blob/main/code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OH7f6H-lQfOA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Import"
      ],
      "metadata": {
        "id": "EOmssdxm3o5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "RI7LFohhh6oy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b804527e-dbe7-4193-9809-3b9e51cc28d6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHx29AkXpS9C",
        "outputId": "707bfaa3-9537-408f-fd9a-0f2ea6d38235"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "id": "ZX3Ibl5BQUFC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import openpyxl\n",
        "from nltk.corpus import stopwords, words, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "import gensim\n",
        "import re, string\n",
        "from string import punctuation, digits\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import one_hot,Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.initializers import Constant\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import ReLU\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense , Flatten ,Embedding,Input,LSTM\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "col_list = [\"Column2\", \"Column3\"]\n",
        "dataset = pd.read_excel(\"Training.data.xlsx\", usecols=col_list)\n",
        "testset = pd.read_excel(\"Dev.xlsx\", usecols=col_list)\n",
        "\n",
        "\n",
        "def add_category_id(dataset):\n",
        "  dataset['category_id'] = dataset['Column3'].factorize()[0]\n",
        "  category_id_dataset = dataset[['Column3', 'category_id']].drop_duplicates()\n",
        "\n",
        "  category_to_id = dict(category_id_dataset.values)\n",
        "  id_to_category = dict(category_id_dataset[['category_id', 'Column3']].values)\n",
        "  return dataset,category_to_id, id_to_category\n",
        "\n",
        "dataset, category_to_id_train, id_to_category_train = add_category_id(dataset)\n",
        "testset, category_to_id_test, id_to_category_test = add_category_id(testset)\n",
        "dataset = dataset.sample(frac = 1)\n",
        "testset = testset.sample(frac = 1)\n",
        "train_x = dataset[\"Column2\"]\n",
        "train_y = dataset[\"category_id\"]\n",
        "test_x = testset[\"Column2\"]\n",
        "test_y = testset[\"category_id\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_x[7412])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yy4EFKrVeZaq",
        "outputId": "cbbe9252-c8b9-4c73-80e4-3c892ec28771"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Today i want to kill myself.... : Because i'm și alone and no one loves me anymore :'(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_tweets = dataset['Column2'].tolist()\n",
        "test_tweets = testset['Column2'].tolist()\n",
        "def keep_uniques(array, df):\n",
        "    dels=[]\n",
        "    for i in array:\n",
        "        if array.count(i)>1:\n",
        "            dels.append(i)\n",
        "    dels=list(set(dels))\n",
        "    for i in dels:\n",
        "        df.drop( df[ df['Column2'] == i ].index, inplace=True)\n",
        "    return df\n",
        "\n",
        "dataset = keep_uniques(train_tweets, dataset)\n",
        "testset = keep_uniques(test_tweets, testset)\n",
        "\n",
        "print(len(testset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_jdBb8Z_Pus",
        "outputId": "4bdd86af-ceb3-45a2-bd83-5498c1b58d84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fig = plt.figure(figsize=(8,6))\n",
        "colors = ['lightblue','blue','darkblue']\n",
        "\n",
        "train_mean = dataset.groupby('Column3').Column2.count().sort_values()\n",
        "print(train_mean)\n",
        "test_mean = testset.groupby('Column3').Column2.count().sort_values()\n",
        "X = ['Antrenare','Validare']\n",
        "severe_mean = [train_mean[0], test_mean[0]]\n",
        "not_mean = [train_mean[1], test_mean[1]]\n",
        "moderate_mean = [train_mean[2], test_mean[2]]\n",
        "X_axis = np.arange(len(X))\n",
        "\n",
        "plt.bar(X_axis , moderate_mean, 0.2, label = 'Moderat',color =\"darkblue\")\n",
        "plt.bar(X_axis + 0.2, not_mean, 0.2, label = 'Fără depresie',color = 'blue')\n",
        "plt.bar(X_axis + 0.2*2, severe_mean, 0.2, label = 'Sever',color = 'lightblue')\n",
        "  \n",
        "plt.xticks(X_axis, X)\n",
        "plt.ylabel(\"Număr de apariții\")\n",
        "plt.title(\"Tweeturi per categorie\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PQ4_yB2Cdnad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PreProcessing"
      ],
      "metadata": {
        "id": "xHx-Yr8O3pvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tweet-preprocessor\n",
        "import preprocessor as p\n",
        "\n",
        "def preprocess_tweets(text):\n",
        "  return p.clean(text)\n",
        "\n",
        "print(preprocess_tweets(train_x[7412]))"
      ],
      "metadata": {
        "id": "i5ORHpDKp2eO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "127eea04-c5e5-4ba4-fb75-c2ccc5902edd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tweet-preprocessor\n",
            "  Downloading tweet_preprocessor-0.6.0-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: tweet-preprocessor\n",
            "Successfully installed tweet-preprocessor-0.6.0\n",
            "Today i want to kill myself.... : Because i'm i alone and no one loves me anymore :'(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:103: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:104: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:105: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:106: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:107: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:108: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:111: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:112: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:113: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:114: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:115: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:116: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        }
      ],
      "source": [
        "# cleaning the text : deleting the emojis, the existing emails, the punctuation, the present digits,\n",
        "# the hyperlinks and the stopwords (a,the,is, etc)\n",
        "\n",
        "def delete_emoji(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               u\"\\U0001f926-\\U0001f937\"\n",
        "                               u\"\\U00010000-\\U0010ffff\"\n",
        "                               u\"\\u2640-\\u2642\"\n",
        "                               u\"\\u2600-\\u2B55\"\n",
        "                               u\"\\u200d\"\n",
        "                               u\"\\u23cf\"\n",
        "                               u\"\\u23e9\"\n",
        "                               u\"\\u231a\"\n",
        "                               u\"\\ufe0f\"  # dingbats\n",
        "                               u\"\\u3030\"\n",
        "                               \"]+\", re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)  # no emoji\n",
        "\n",
        "\n",
        "def delete_digits(text):\n",
        "    text = text.lower()\n",
        "    clean = text.translate(str.maketrans('', '', digits))\n",
        "    return clean\n",
        "\n",
        "\n",
        "def delete_punctuation(text):\n",
        "    clean = text.translate(str.maketrans('', '', punctuation + '’“”'))\n",
        "    return clean\n",
        "\n",
        "\n",
        "stop = set(stopwords.words('english'))\n",
        "stop.add('gtpoplt')\n",
        "stop.add('new')\n",
        "stop.add('year')\n",
        "stop.add('eve')\n",
        "stop.add('years')\n",
        "stop.add('ti')\n",
        "stop.add('ame')\n",
        "stop.add('folks')\n",
        "punct = list(string.punctuation)\n",
        "stop.update(punct)\n",
        "\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    final_text = []\n",
        "    for i in text.split():\n",
        "        if i.strip().lower() not in stop:\n",
        "            final_text.append(i.strip())\n",
        "    return \" \".join(final_text)\n",
        "\n",
        "\n",
        "def remove_hyperlinks(text):\n",
        "    for word in text:\n",
        "        if re.match(r'^http', word):\n",
        "            text.remove(word)\n",
        "    return \" \".join(text)\n",
        "\n",
        "def remove_emails(text):\n",
        "    text = text.split()\n",
        "    for i in text:\n",
        "        if '@' in i.strip().lower():\n",
        "            text.remove(i)\n",
        "    return \" \".join(text)\n",
        "\n",
        "contraction_dict = {\"youre\":\"you are\",\"im\": \"i am\",\"wouldnt\": \"would not\",\"itll\": \"it will\",\"wasnt\": \"was not\",\"dont\": \"do not\",\"ill\": \"i will\",\"isnt\": \"is not\",\"cant\": \"cannot\",\"arent\": \"are not\",\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
        "def _get_contractions(contraction_dict):\n",
        "    contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n",
        "    return contraction_dict, contraction_re\n",
        "contractions, contractions_re = _get_contractions(contraction_dict)\n",
        "def replace_contractions(text):\n",
        "    def replace(match):\n",
        "        return contractions[match.group(0)]\n",
        "    return contractions_re.sub(replace, text)\n",
        "\n",
        "# def denoise_text(text):\n",
        "#     text = replace_contractions(text)\n",
        "#     text = remove_words(text)\n",
        "#     text = remove_emails(text)\n",
        "#     text = delete_punctuation(text)\n",
        "#     text = delete_emoji(text)\n",
        "#     text = delete_digits(text)\n",
        "#     text = remove_stopwords(text)\n",
        "#     text = text.split()\n",
        "#     text = remove_hyperlinks(text)\n",
        "#     return text\n",
        "\n",
        "\n",
        "\n",
        "train_x = dataset[\"Column2\"]\n",
        "train_y = dataset[\"category_id\"]\n",
        "test_x = testset[\"Column2\"]\n",
        "test_y = testset[\"category_id\"]\n",
        "\n",
        "for i in range(len(train_x)):\n",
        "  train_x[i] = replace_contractions(train_x[i])\n",
        "  train_x[i] = remove_emails(train_x[i])\n",
        "  train_x[i] = delete_punctuation(train_x[i])\n",
        "  train_x[i] = delete_emoji(train_x[i])\n",
        "  train_x[i] = delete_digits(train_x[i])\n",
        "  train_x[i] = remove_stopwords(train_x[i])\n",
        "\n",
        "for i in range(len(test_x)):\n",
        "  test_x[i] = replace_contractions(test_x[i])\n",
        "  test_x[i] = remove_emails(test_x[i])\n",
        "  test_x[i] = delete_punctuation(test_x[i])\n",
        "  test_x[i] = delete_emoji(test_x[i])\n",
        "  test_x[i] = delete_digits(test_x[i])\n",
        "  test_x[i] = remove_stopwords(test_x[i])\n",
        "\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "fySiVht3QUFH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "959eaca2-205b-4567-98a3-5dfe052f6bf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_x[7449])\n",
        "print(dataset[\"Column2\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfI378zkgRMG",
        "outputId": "1bdd3e97-315a-42e5-d4d3-89f5295a008b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "happy started cutting wrists like feel way never done feel ashamed old think speak friends husband feel lost alone know\n",
            "6540    school makes suicidal highschool think gonna a...\n",
            "4456    fuck titles depressed feel fucking worthless a...\n",
            "4871    gonna kick ass end wi miracle ive already spir...\n",
            "288     scared hurting get drunk turn couple days feel...\n",
            "5930    happy today start goals ambitions experiences ...\n",
            "                              ...                        \n",
            "1301    spending nye alone car sucked say least homele...\n",
            "8034    long mirtazapine side effects last throwaway a...\n",
            "5865    anyone wanna talk hey yall anyone busy dm sis ...\n",
            "7130    ever provided therapy clients suicidality deat...\n",
            "1656    party want cry panic attack without reason any...\n",
            "Name: Column2, Length: 8068, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatization"
      ],
      "metadata": {
        "id": "cfS5G2jQ4GGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayod5PmNYU7H",
        "outputId": "71fb9304-9521-4ee8-ebb9-9ba178ff6c7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/omw-1.4.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# lemmatization : crying -> cry, days -> day\n",
        "\n",
        "def get_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "\n",
        "def lemm(text):\n",
        "    text = text.split()\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    sar_list_lemmatizer = [lemmatizer.lemmatize(word, get_pos(word)) for word in text]\n",
        "    return \" \".join(sar_list_lemmatizer)\n",
        "\n",
        "train_x = train_x.apply(lemm)\n",
        "test_x = test_x.apply(lemm)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "3oSwfDyzQUFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_x[7429])"
      ],
      "metadata": {
        "id": "0Gst2fP9GIvO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d43e5bd4-2a32-4980-c019-ce263745a4c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "truly wish someone would shoot hope die think life past like always there get away except death\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "text = ' '.join(train_x[dataset['Column3']=='severe'])\n",
        "wc = WordCloud(background_color='white').generate(text)\n",
        "plt.imshow(wc)"
      ],
      "metadata": {
        "id": "t0Oao20MI9Mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Count Vectorizer"
      ],
      "metadata": {
        "id": "IMHPbMBI4VTQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8068, 10252)\n"
          ]
        }
      ],
      "source": [
        "# CountVectorizer reprezentation for the user tweets\n",
        "vectorizer = CountVectorizer()\n",
        "x_train_cv = vectorizer.fit_transform(train_x)\n",
        "\n",
        "x_test_cv = vectorizer.transform(test_x)\n",
        "print(x_train_cv.shape)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "8blt8KSfQUFd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e82e995-770d-43cf-9733-391e0429529d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data imbalance handling"
      ],
      "metadata": {
        "id": "7MBdfMwk4bR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sm = SMOTE(random_state = 42)\n",
        "res_x, res_y = sm.fit_resample(x_train_cv, train_y)\n",
        "print(\"After OverSampling, counts of label '2': {}\".format(sum(res_y == 2)))"
      ],
      "metadata": {
        "id": "w-x13nL-a0EM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "935333c4-69ee-48cb-9e18-b10b326c365b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After OverSampling, counts of label '2': 5647\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os = RandomOverSampler()\n",
        "res_x2, res_y2 = os.fit_resample(x_train_cv, train_y)"
      ],
      "metadata": {
        "id": "020ZvbDV8Yk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tf-Idf "
      ],
      "metadata": {
        "id": "2abJM4PN4yyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tf-idf reprezentation for the tweets\n",
        "from sklearn.feature_selection import chi2\n",
        "vectorizer = TfidfVectorizer(max_df = 0.15,min_df = 5, ngram_range=(1,2), stop_words='english')\n",
        "train_x_tf = vectorizer.fit_transform(train_x)\n",
        "test_x_tf = vectorizer.transform(test_x)\n",
        "N = 2\n",
        "# for Product, category_id in sorted(category_to_id_train.items()):\n",
        "#     features_chi2 = chi2(train_x_tf, train_y == category_id)\n",
        "#     indices = np.argsort(features_chi2[0])\n",
        "#     feature_names = np.array(vectorizer.get_feature_names())[indices]\n",
        "#     unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
        "#     bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
        "#     print(\"# '{}':\".format(Product))\n",
        "#     print(\" Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-N:])))\n",
        "#     print(\" Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-N:])))\n",
        "res_tfx, res_tfy = sm.fit_resample(train_x_tf, train_y)\n",
        "print(train_x_tf.shape)\n",
        "print(res_tfx.shape)"
      ],
      "metadata": {
        "id": "WkoI7tMeRnXz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e2e9b9f-567d-464c-a539-9d76f4e02498"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8068, 33909)\n",
            "(16941, 33909)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word2Vec"
      ],
      "metadata": {
        "id": "5_goD7gM5Bnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent = [row.split() for row in train_x]"
      ],
      "metadata": {
        "id": "Q_TNLIZYk8DA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.phrases import Phrases, Phraser\n"
      ],
      "metadata": {
        "id": "wH95fWT2lHE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram = Phraser(phrases)\n",
        "sentences = bigram[sent]"
      ],
      "metadata": {
        "id": "ffE5uhQflNF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentences)"
      ],
      "metadata": {
        "id": "xslKOh8bnezF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model = Word2Vec(min_count=20,\n",
        "                     window=5,\n",
        "                     size=300,\n",
        "                     sample=6e-5, \n",
        "                     alpha=0.03, \n",
        "                     min_alpha=0.0007, \n",
        "                     negative=20,\n",
        "                     )"
      ],
      "metadata": {
        "id": "XP-cfBanlg4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.build_vocab(sentences)"
      ],
      "metadata": {
        "id": "FjD343pblz0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHmC66qBl6T1",
        "outputId": "9a10147b-62c4-48a5-ef56-28f41040b0a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5675934, 16754460)"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.wv.most_similar(positive=[\"depression\"])"
      ],
      "metadata": {
        "id": "rSI6JFcmmM43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word2(train_x):\n",
        "  mes = []\n",
        "  for i in train_x:\n",
        "    mes.append(i.split())\n",
        "  word2vec_model = Word2Vec(mes, size=300, window=5, min_count=1)\n",
        "  token = Tokenizer()\n",
        "  token.fit_on_texts(mes)\n",
        "  text = token.texts_to_sequences(mes)\n",
        "  text = pad_sequences(text, 1464)\n",
        "  word_index = token.word_index\n",
        "  return word2vec_model,word_index,text"
      ],
      "metadata": {
        "id": "eSgyTFdxu0dX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model,word_index,pad_rev_train = word2(train_x)"
      ],
      "metadata": {
        "id": "eJRUq5f6v1FO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2,word_index2,pad_rev_test = word2(test_x)"
      ],
      "metadata": {
        "id": "kwKmyoLL0HF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.wv.most_similar(\"sex\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoP4ZQ7kv7HW",
        "outputId": "1d7e6aa1-83ff-4df0-e7b2-e8260113812d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('available', 0.9989280700683594),\n",
              " ('circle', 0.9982731342315674),\n",
              " ('confess', 0.9981011152267456),\n",
              " ('woman', 0.9979820847511292),\n",
              " ('business', 0.9978442192077637),\n",
              " ('host', 0.9977796673774719),\n",
              " ('bond', 0.997644305229187),\n",
              " ('contact', 0.9974455237388611),\n",
              " ('argue', 0.997405469417572),\n",
              " ('manipulate', 0.9973846673965454)]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_weight_matrix(model, vocab):\n",
        "    # total vocabulary size plus 0 for unknown words\n",
        "    vocab_size = len(vocab) + 1\n",
        "    # define weight matrix dimensions with all 0\n",
        "    weight_matrix = np.zeros((vocab_size, 300))\n",
        "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
        "    for word, i in vocab.items():\n",
        "        weight_matrix[i] = model[word]\n",
        "    return weight_matrix"
      ],
      "metadata": {
        "id": "O25ga7HBxIk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_vectors = get_weight_matrix(model, word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBosyRN8xMkY",
        "outputId": "3f3fce21-2afe-4a79-aa28-5024f4947cfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def w2v_embedding(train_text):\n",
        "  ## create list of lists of unigrams\n",
        "  lst_corpus = []\n",
        "  for string in train_text:\n",
        "    lst_words = string.split()\n",
        "    lst_grams = [\" \".join(lst_words[i:i+1]) \n",
        "                for i in range(0, len(lst_words), 1)]\n",
        "    lst_corpus.append(lst_grams)\n",
        "\n",
        "  ## detect bigrams and trigrams\n",
        "  bigrams_detector = gensim.models.phrases.Phrases(lst_corpus, \n",
        "                  delimiter=\" \".encode(), min_count=30)\n",
        "  bigrams_detector = gensim.models.phrases.Phraser(bigrams_detector)\n",
        "  trigrams_detector = gensim.models.phrases.Phrases(bigrams_detector[lst_corpus], \n",
        "              delimiter=\" \".encode(), min_count=30)\n",
        "  trigrams_detector = gensim.models.phrases.Phraser(trigrams_detector)\n",
        "\n",
        "  nlp = Word2Vec(lst_corpus, size=300,   \n",
        "            window=8, min_count=2, sg=1, iter=30)\n",
        "  return nlp, lst_corpus, bigrams_detector, trigrams_detector"
      ],
      "metadata": {
        "id": "YqW8Yiqr4vEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_engineering(lst_corpus, train_text):\n",
        "  tokenizer = Tokenizer(lower=True, split=' ', \n",
        "                     oov_token=\"NaN\", \n",
        "                     filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
        "  tokenizer.fit_on_texts(lst_corpus)\n",
        "  dic_vocabulary = tokenizer.word_index\n",
        "  ## create sequence\n",
        "  lst_corpus = []\n",
        "  for string in train_text:\n",
        "    lst_words = string.split()\n",
        "    lst_grams = [\" \".join(lst_words[i:i+1]) \n",
        "                for i in range(0, len(lst_words), 1)]\n",
        "    lst_corpus.append(lst_grams)\n",
        "  lst_text2seq= tokenizer.texts_to_sequences(lst_corpus)\n",
        "  ## padding sequence\n",
        "  X_train = pad_sequences(lst_text2seq, \n",
        "                      maxlen=30, padding=\"post\", truncating=\"post\")\n",
        "  return tokenizer, dic_vocabulary, X_train"
      ],
      "metadata": {
        "id": "HXONvgmg40qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_embedding_matrix(dic_vocabulary, nlp):\n",
        "  ## start the matrix (length of vocabulary x vector size) with all 0s\n",
        "  embeddings = np.zeros((len(dic_vocabulary)+1, 300))\n",
        "  for word,idx in dic_vocabulary.items():\n",
        "      ## update the row with vector\n",
        "      try:\n",
        "          embeddings[idx] =  nlp[word]\n",
        "      ## if word not in model then skip and the row stays all 0s\n",
        "      except:\n",
        "          pass\n",
        "  return embeddings"
      ],
      "metadata": {
        "id": "w3XdYm2A421r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp, lst_corpus, bigrams_detector, trigrams_detector = w2v_embedding(train_x)"
      ],
      "metadata": {
        "id": "eYbWGZel4488"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(nlp.wv.most_similar(positive=\"depression\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WIjrfeL9fYT",
        "outputId": "dcfc0b8a-29a7-448d-93e7-30ec3ea94d53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('ocdanxiety', 0.3492339253425598), ('anxiety', 0.3416626453399658), ('diagnose', 0.33797287940979004), ('longstanding', 0.3372364044189453), ('actor', 0.33621853590011597), ('stabilizer', 0.334298312664032), ('crucial', 0.33266133069992065), ('microdosing', 0.3280254006385803), ('selenium', 0.32584166526794434), ('sought', 0.32460087537765503)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer, dic_vocabulary, X_train = feature_engineering(lst_corpus,train_x)"
      ],
      "metadata": {
        "id": "4O1vFy469Jaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYBf8fPNBzrc",
        "outputId": "16984993-2e68-42e3-cdc1-6b5c6395e7fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8068, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_train = make_embedding_matrix(dic_vocabulary, nlp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3JJiPOE9QLZ",
        "outputId": "7d1510ac-78d8-475f-bf6c-503216737d73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  import sys\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_handling(test_text,bigrams_detector, trigrams_detector, tokenizer):\n",
        "  ## create list of n-grams\n",
        "  lst_corpus = []\n",
        "  for string in test_text:\n",
        "      lst_words = string.split()\n",
        "      lst_grams = [\" \".join(lst_words[i:i+1]) for i in range(0, \n",
        "                  len(lst_words), 1)]\n",
        "      lst_corpus.append(lst_grams)\n",
        "      \n",
        "  ## detect common bigrams and trigrams using the fitted detectors\n",
        "  lst_corpus = list(bigrams_detector[lst_corpus])\n",
        "  lst_corpus = list(trigrams_detector[lst_corpus])\n",
        "  ## text to sequence with the fitted tokenizer\n",
        "  lst_text2seq = tokenizer.texts_to_sequences(lst_corpus)\n",
        "\n",
        "  ## padding sequence\n",
        "  X_test = pad_sequences(lst_text2seq, maxlen=30,\n",
        "              padding=\"post\", truncating=\"post\")\n",
        "  return X_test"
      ],
      "metadata": {
        "id": "iH5BAwFM_F8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp2, lst_corpus2, bigrams_detector2, trigrams_detector2 = w2v_embedding(test_x)"
      ],
      "metadata": {
        "id": "JGH7zCw3C3mA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = test_handling(test_x, bigrams_detector2, trigrams_detector2, tokenizer)"
      ],
      "metadata": {
        "id": "P7S9fXX7-E3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IH1zz40QCuUi",
        "outputId": "a974d23a-a64f-433d-dc6f-ff15ae4432ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4078, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes"
      ],
      "metadata": {
        "id": "KsTCPqZS4jzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "naive_bayes_classifier = MultinomialNB()\n",
        "naive_bayes_classifier.fit(res_x, res_y)\n",
        "pred_y = naive_bayes_classifier.predict(x_test_cv)\n",
        "\n",
        "score1 = metrics.accuracy_score(test_y, pred_y)\n",
        "print(\"Accuracy \" + str(score1))\n",
        "print(metrics.classification_report(test_y, pred_y, digits = 4))"
      ],
      "metadata": {
        "id": "uDe2Ts7ZgnZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "naive_bayes_classifier = MultinomialNB()\n",
        "naive_bayes_classifier.fit(res_tfx, res_tfy)\n",
        "# naive_bayes_classifier.fit(train_x_tf, train_y)\n",
        "pred_y = naive_bayes_classifier.predict(test_x_tf)\n",
        "score1 = metrics.accuracy_score(test_y, pred_y)\n",
        "print(\"Accuracy \" + str(score1))\n",
        "print(metrics.classification_report(test_y, pred_y, digits = 4))"
      ],
      "metadata": {
        "id": "UVkenp6V8Qce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AdaBoost"
      ],
      "metadata": {
        "id": "cWK3dbEM45Fj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# res_x2, res_y2 = sm.fit_resample(train_x_tf, train_y)\n",
        "ada = AdaBoostClassifier()\n",
        "\n",
        "# boost = ada.fit(train_x_tf, train_y)\n",
        "boost = ada.fit(res_tfx, res_tfy)\n",
        "pred_y = boost.predict(test_x_tf)\n",
        "print(\"AdaBoost Classifier Model Accuracy:\", accuracy_score(test_y, pred_y))\n",
        "print(metrics.classification_report(test_y, pred_y))"
      ],
      "metadata": {
        "id": "6QDGISapwPVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression"
      ],
      "metadata": {
        "id": "Kz3D-3YoDine"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_weight = {0: 1.,\n",
        "                1: 3.,\n",
        "                2: 4.}\n",
        "\n",
        "model = LogisticRegression(multi_class='multinomial', solver='newton-cg')\n",
        "# cv = RepeatedStratifiedKFold(n_splits = 10, n_repeats=3)\n",
        "model = model.fit(res_x, res_y)\n",
        "y_pred = model.predict(x_test_cv)\n",
        "\n",
        "# model = model.fit(res_tfx, res_tfy)\n",
        "# y_pred = model.predict(test_x_tf)\n",
        "\n",
        "print(\"Regression Classifier Model Accuracy:\", accuracy_score(test_y, y_pred))\n",
        "print(metrics.classification_report(test_y, y_pred, digits = 4))\n",
        "\n",
        "conf_mat = confusion_matrix(test_y, y_pred)\n",
        "print(conf_mat)\n"
      ],
      "metadata": {
        "id": "hKYzd_kPDmDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class_weight = {0: 1.,\n",
        "                1: 3.,\n",
        "                2: 6.}\n",
        "model = LinearSVC()\n",
        "# model = model.fit(train_x_tf, train_y)\n",
        "# y_pred = model.predict(test_x_tf)\n",
        "model = model.fit(res_x, res_y)\n",
        "y_pred = model.predict(x_test_cv)\n",
        "print(\"Svc Classifier Model Accuracy:\", accuracy_score(test_y, y_pred))\n",
        "print(metrics.classification_report(test_y, y_pred, digits = 4))\n",
        "\n",
        "conf_mat = confusion_matrix(test_y, y_pred)\n",
        "print(conf_mat)\n"
      ],
      "metadata": {
        "id": "wDRy1f9pv-I1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class_weight = {0: 1.,\n",
        "                1: 3.,\n",
        "                2: 5.}\n",
        "model = RandomForestClassifier(class_weight=class_weight)\n",
        "model = model.fit(train_x_tf,train_y)\n",
        "y_pred = model.predict(test_x_tf)\n",
        "print(\"RandomForest Classifier Model Accuracy:\", accuracy_score(test_y, y_pred))\n",
        "print(metrics.classification_report(test_y, y_pred))\n",
        "\n",
        "conf_mat = confusion_matrix(test_y, y_pred)\n",
        "print(conf_mat)"
      ],
      "metadata": {
        "id": "aPLXrd2Cb32y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# F1 metric"
      ],
      "metadata": {
        "id": "lgvLSOR8gsOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def f1(y_true, y_pred):\n",
        "    y_pred = K.round(y_pred)\n",
        "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
        "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
        "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
        "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
        "\n",
        "    p = tp / (tp + fp + K.epsilon())\n",
        "    r = tp / (tp + fn + K.epsilon())\n",
        "\n",
        "    f1 = 2*p*r / (p+r+K.epsilon())\n",
        "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
        "    return K.mean(f1)\n",
        "\n",
        "def f1_loss(y_true, y_pred):\n",
        "    \n",
        "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
        "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
        "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
        "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
        "\n",
        "    p = tp / (tp + fp + K.epsilon())\n",
        "    r = tp / (tp + fn + K.epsilon())\n",
        "\n",
        "    f1 = 2*p*r / (p+r+K.epsilon())\n",
        "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
        "    return 1 - K.mean(f1)"
      ],
      "metadata": {
        "id": "bdQgAL5I6cCW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network"
      ],
      "metadata": {
        "id": "tUiQnNNV5Frg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_weight = {0: 1.,\n",
        "                1: 3.,\n",
        "                2: 5.}\n",
        "# def split(pad_rev,train_y):\n",
        "#   Y=keras.utils.to_categorical(train_y)  # one hot target as required by NN.\n",
        "#   x_train,x_test,y_train,y_test=train_test_split(pad_rev,Y,test_size=0.05,random_state=42)\n",
        "#   return x_train, x_test,y_train,y_test\n",
        "def network(embeddings):\n",
        " ## input\n",
        "  x_in = layers.Input(shape=(30,))\n",
        "  ## embedding\n",
        "  x = layers.Embedding(input_dim=embeddings.shape[0],  \n",
        "                      output_dim=embeddings.shape[1], \n",
        "                      weights=[embeddings],\n",
        "                      input_length=30, trainable=False)(x_in)\n",
        "  ## 2 layers of bidirectional lstm\n",
        "  x = layers.Bidirectional(layers.LSTM(units=30, dropout=0.2, \n",
        "                          return_sequences=True))(x)\n",
        "  x = layers.Bidirectional(layers.LSTM(units=30, dropout=0.2))(x)\n",
        "  ## final dense layers\n",
        "  x = layers.Dense(64, activation='elu')(x)\n",
        "  y_out = layers.Dense(3, activation='softmax')(x)\n",
        "  ## compile\n",
        "  model = Model(x_in, y_out)\n",
        "  model.compile(loss=f1_loss,\n",
        "                optimizer='adam', metrics=[f1])\n",
        "  return model\n",
        "Y=keras.utils.to_categorical(train_y)\n",
        "Y2=keras.utils.to_categorical(test_y)\n",
        "# x_train,x_test,y_train,y_test = split(X_train,train_y)\n",
        "model = network(embeddings_train)\n",
        "# x_valid, x_x, y_valid, y_y = split(x_test,test_y)\n",
        "model.fit(X_train, Y, epochs=10, batch_size=64, validation_data=(x_test,Y2))\n"
      ],
      "metadata": {
        "id": "QAeWwiGyMJFL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1215a47-bd0a-47cb-a573-1fad21fbfc13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "127/127 [==============================] - 15s 27ms/step - loss: 0.5432 - f1: 0.4327 - val_loss: 0.5343 - val_f1: 0.4707\n",
            "Epoch 2/10\n",
            "127/127 [==============================] - 2s 16ms/step - loss: 0.3959 - f1: 0.6105 - val_loss: 0.5172 - val_f1: 0.4860\n",
            "Epoch 3/10\n",
            "127/127 [==============================] - 2s 16ms/step - loss: 0.3529 - f1: 0.6526 - val_loss: 0.5382 - val_f1: 0.4687\n",
            "Epoch 4/10\n",
            "127/127 [==============================] - 2s 16ms/step - loss: 0.3149 - f1: 0.6864 - val_loss: 0.5477 - val_f1: 0.4541\n",
            "Epoch 5/10\n",
            "127/127 [==============================] - 2s 16ms/step - loss: 0.3124 - f1: 0.6872 - val_loss: 0.5817 - val_f1: 0.4206\n",
            "Epoch 6/10\n",
            "127/127 [==============================] - 2s 16ms/step - loss: 0.3093 - f1: 0.6919 - val_loss: 0.5496 - val_f1: 0.4484\n",
            "Epoch 7/10\n",
            "127/127 [==============================] - 2s 16ms/step - loss: 0.2902 - f1: 0.7118 - val_loss: 0.5554 - val_f1: 0.4478\n",
            "Epoch 8/10\n",
            "127/127 [==============================] - 2s 16ms/step - loss: 0.2751 - f1: 0.7298 - val_loss: 0.5496 - val_f1: 0.4499\n",
            "Epoch 9/10\n",
            "127/127 [==============================] - 2s 16ms/step - loss: 0.2659 - f1: 0.7328 - val_loss: 0.5257 - val_f1: 0.4739\n",
            "Epoch 10/10\n",
            "127/127 [==============================] - 2s 16ms/step - loss: 0.2589 - f1: 0.7422 - val_loss: 0.5236 - val_f1: 0.4770\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb4de1ea650>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z922lilzZ5KH",
        "outputId": "ff76f7da-ded7-4f66-9e0b-9979b6a19ea0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 30)]              0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 30, 300)           3101700   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 30, 60)           79440     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 60)               21840     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                3904      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3)                 195       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,207,079\n",
            "Trainable params: 105,379\n",
            "Non-trainable params: 3,101,700\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_y = model.predict(x_test)\n",
        "pred_y = np.argmax(pred_y, axis=1)\n",
        "# y_test=np.argmax(y_test, axis=1)\n",
        "print(metrics.classification_report(test_y, pred_y, digits = 4))\n",
        "\n",
        "conf_mat = confusion_matrix(test_y, pred_y)\n",
        "print(conf_mat)"
      ],
      "metadata": {
        "id": "tHB9SVT5O4On",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5da3d19e-0a7f-40f9-cddf-78df0e1bb6a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5965    0.7944    0.6814      2198\n",
            "           1     0.5712    0.2792    0.3751      1522\n",
            "           2     0.3759    0.4274    0.4000       358\n",
            "\n",
            "    accuracy                         0.5699      4078\n",
            "   macro avg     0.5146    0.5003    0.4855      4078\n",
            "weighted avg     0.5677    0.5699    0.5424      4078\n",
            "\n",
            "[[1746  291  161]\n",
            " [1004  425   93]\n",
            " [ 177   28  153]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT"
      ],
      "metadata": {
        "id": "fccPGYtiwkXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "hdP6B25p2WM6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58be628f-0931-4c35-9154-45ba96341e21"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.20.0-py3-none-any.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 4.9 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 49.5 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 62.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 6.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.20.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "OOyweo6YFNFC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "102cee15-facb-497f-d9fb-24b968bf00e6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence_transformers\n",
            "  Downloading sentence-transformers-2.2.0.tar.gz (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 727 kB/s \n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.20.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.64.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.11.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.12.0+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (3.7)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 10.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.7.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence_transformers) (4.2.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (4.11.4)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.12.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (3.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.6.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.8.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (7.1.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.24.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence_transformers) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.0-py3-none-any.whl size=120747 sha256=9c83ca6b9dfb2a647a064c9d6fb8b10d863dd666571cb0ea5745357b309ab745\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/c0/df/b6873ab7aac3f2465aa9144b6b4c41c4391cfecc027c8b07e7\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentencepiece, sentence-transformers\n",
            "Successfully installed sentence-transformers-2.2.0 sentencepiece-0.1.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.initializers import GlorotNormal\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "gr1JKdNUF9yM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizer"
      ],
      "metadata": {
        "id": "n_LErOdPgPJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
      ],
      "metadata": {
        "id": "ptBxQVHFgah4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_train = train_x.astype(str).tolist()\n",
        "text_test = test_x.astype(str).tolist()"
      ],
      "metadata": {
        "id": "PsYlNr3RgtrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = tokenizer.tokenize(train_x)\n",
        "embt = tokenizer.encode(s)"
      ],
      "metadata": {
        "id": "A7u9Cgbygh0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizerFast, RobertaForSequenceClassification,Trainer, TrainingArguments"
      ],
      "metadata": {
        "id": "V7Ld9eDDhqt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RobertaForSequenceClassification.from_pretrained('roberta-base')\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base', max_length = 512)"
      ],
      "metadata": {
        "id": "XuvlQftshfwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "QB_CWjoLjM6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas"
      ],
      "metadata": {
        "id": "fbAPquW4kHt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install load_dataframe"
      ],
      "metadata": {
        "id": "aVdi83dlkCWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "rllg8s-BkiQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = pd.load_dataframe(\"data1\", \"Training.data.xlsx\", \"¦\", \"utf-8\")\n",
        "data_test = pd.load_dataframe(\"data1\", \"Dev.xlsx\", \"¦\", \"utf-8\")\n",
        "data_train=data_train[\"train\"]\n",
        "data_test=data_test[\"train\"]"
      ],
      "metadata": {
        "id": "JoZ_VOFrjiYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "from datasets import ClassLabel, Value\n",
        "print(type(train_x))\n",
        "data_train = train_x.select(range(1,len(train_x)))\n",
        "new_features = data_train.features.copy()\n",
        "new_features[\"label\"] = Value('int32')\n",
        "data_train = data_train.cast(new_features)\n",
        "print(data_train.features)"
      ],
      "metadata": {
        "id": "dWAyGgUiiOYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenization(batched_text):\n",
        "    return tokenizer(batched_text['text'], padding = True, truncation=True)\n",
        "\n",
        "\n",
        "data_train = train_x.map(tokenization)\n",
        "data_test = test_x.map(tokenization)"
      ],
      "metadata": {
        "id": "lCst4s0kh41z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer(\"all-mpnet-base-v2\")"
      ],
      "metadata": {
        "id": "cz2XLEkFgy2x"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence(texts):\n",
        "  model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
        "  embeddings = model.encode(texts)\n",
        "\n",
        "  return embeddings"
      ],
      "metadata": {
        "id": "IBHUesv8FbMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRANSFORMER_BATCH=128\n",
        "\n",
        "def count_embedd (df):\n",
        "    idx_chunk=list(df.columns).index('Column2')\n",
        "    embedd_lst = []\n",
        "    for index in range (0, len(df), TRANSFORMER_BATCH):\n",
        "        embedds = model.encode(df.iloc[index:index+TRANSFORMER_BATCH, idx_chunk].values, show_progress_bar=False)\n",
        "        embedd_lst.append(embedds)\n",
        "    return np.concatenate(embedd_lst)"
      ],
      "metadata": {
        "id": "HIZk6hn2aiu_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_emb = count_embedd(dataset)"
      ],
      "metadata": {
        "id": "jyItDXSMgnog"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_emb = count_embedd(testset)"
      ],
      "metadata": {
        "id": "-4U0N7MSg4Wr"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.array(train_emb)\n",
        "X_test = np.array(test_emb)"
      ],
      "metadata": {
        "id": "CCkIOGTrbp2s"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train=keras.utils.to_categorical(train_y)"
      ],
      "metadata": {
        "id": "tf8ht-tgb_n_"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KERAS_VALIDATION_SPLIT=0.00\n",
        "KERAS_EPOCHS=10\n",
        "KERAS_BATCH_SIZE=128\n",
        "\n",
        "class_weight = {0: 1.,\n",
        "                1: 3.,\n",
        "                2: 6.}\n",
        "\n",
        "# Create and train Keras model\n",
        "n_features=X_train.shape[1]\n",
        "n_labels = y_train.shape[1]\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Dense(2048, input_dim=n_features),\n",
        "  tf.keras.layers.LeakyReLU(alpha=0.01),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(64),\n",
        "  tf.keras.layers.LeakyReLU(alpha=0.01),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "LR=5e-4\n",
        "adam = keras.optimizers.Adam(learning_rate=LR, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "\n",
        "model.compile(optimizer=adam, \n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[f1])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=KERAS_EPOCHS, batch_size=KERAS_BATCH_SIZE, validation_split=KERAS_VALIDATION_SPLIT)"
      ],
      "metadata": {
        "id": "HiKl8FLGb3ju",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03fcf3a2-aa61-4c40-9390-642f89189987"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "64/64 [==============================] - 1s 5ms/step - loss: 0.7079 - f1: 0.3424\n",
            "Epoch 2/10\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.5525 - f1: 0.5200\n",
            "Epoch 3/10\n",
            "64/64 [==============================] - 0s 5ms/step - loss: 0.4560 - f1: 0.6754\n",
            "Epoch 4/10\n",
            "64/64 [==============================] - 0s 5ms/step - loss: 0.3640 - f1: 0.7782\n",
            "Epoch 5/10\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.3026 - f1: 0.8254\n",
            "Epoch 6/10\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.2511 - f1: 0.8491\n",
            "Epoch 7/10\n",
            "64/64 [==============================] - 0s 4ms/step - loss: 0.2153 - f1: 0.8777\n",
            "Epoch 8/10\n",
            "64/64 [==============================] - 0s 5ms/step - loss: 0.1836 - f1: 0.8931\n",
            "Epoch 9/10\n",
            "64/64 [==============================] - 0s 5ms/step - loss: 0.1442 - f1: 0.9234\n",
            "Epoch 10/10\n",
            "64/64 [==============================] - 0s 5ms/step - loss: 0.1206 - f1: 0.9347\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7efc7a74f0d0>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()\n"
      ],
      "metadata": {
        "id": "ZZ2HfzDYboIH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67195c62-81eb-4914-acae-651ecfb7ebb2"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_6 (Dense)             (None, 2048)              1574912   \n",
            "                                                                 \n",
            " leaky_re_lu_4 (LeakyReLU)   (None, 2048)              0         \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 64)                131136    \n",
            "                                                                 \n",
            " leaky_re_lu_5 (LeakyReLU)   (None, 64)                0         \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 3)                 195       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,706,243\n",
            "Trainable params: 1,706,243\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_preds = model.predict(X_test)\n",
        "predicted = np.argmax(y_preds,axis=1) \n",
        "accuracy = metrics.accuracy_score(test_y, predicted)\n",
        "print(\"Accuracy:\",  round(accuracy,4))\n",
        "print(metrics.classification_report(test_y, predicted,digits=4))\n",
        "\n",
        "conf_mat = confusion_matrix(test_y, predicted)\n",
        "print(conf_mat)"
      ],
      "metadata": {
        "id": "vyOx-aVqcLte",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a194cb8-fc31-4207-ecbe-6ceea4779e2f"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.565\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6050    0.7170    0.6563      2198\n",
            "           1     0.5664    0.3837    0.4575      1522\n",
            "           2     0.3258    0.4022    0.3600       358\n",
            "\n",
            "    accuracy                         0.5650      4078\n",
            "   macro avg     0.4991    0.5010    0.4913      4078\n",
            "weighted avg     0.5661    0.5650    0.5561      4078\n",
            "\n",
            "[[1576  415  207]\n",
            " [ 847  584   91]\n",
            " [ 182   32  144]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT2"
      ],
      "metadata": {
        "id": "bYvS_CNvA0qU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzBSzy1OIDUK",
        "outputId": "c020e04a-4d09-4dcb-acbc-06e11e1b85ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/content/drive\")\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "2aLjBQPKIFVg",
        "outputId": "54837b04-e9c5-4c71-ac25-cb5e84fa822b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-44c501624541>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/content/drive\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ls'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/content/drive'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from transformers import AutoModel, AdamW, AutoTokenizer, get_scheduler\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset, RandomSampler\n",
        "import torch\n",
        "from sklearn.utils import class_weight\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import f1_score"
      ],
      "metadata": {
        "id": "iitfB3WWA4mJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_config = {\n",
        "    \"model_name\": \"cardiffnlp/twitter-roberta-base-sentiment\",\n",
        "    # \"path\":\"/content/drive/MyDrive/content/drive/models/twitter-roberta-base-sentiment-new\",\n",
        "    \"max_length\":40,\n",
        "    \"batch_size\":16,\n",
        "    \"source\":\"HuggingFace\"\n",
        "}"
      ],
      "metadata": {
        "id": "kYbWpL9gA6Ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_config = {\n",
        "    \"learning_rate\" : 2e-5,\n",
        "    \"weight_decay\":0.01,\n",
        "    \"epochs\":20\n",
        "}"
      ],
      "metadata": {
        "id": "PCm-p3_zA74b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "8MlTN7XeA9GP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights = class_weight.compute_class_weight(class_weight=\"balanced\", classes = np.unique(dataset[\"Column3\"]), y=np.array(dataset[\"Column3\"]))\n",
        "print(class_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDWyhE_XA-k3",
        "outputId": "1c62fba3-9ed3-4531-d00a-b7df396173f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.47624107 1.71842386 3.14174455]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BertClassfierPytorch(nn.Module):\n",
        "  def __init__(self, input_size = 768, output_size = 3):\n",
        "    super().__init__()\n",
        "    print(input_size)\n",
        "    self.name = embedding_config[\"model_name\"]\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(embedding_config[\"model_name\"], use_fast=False)\n",
        "    self.embedder=AutoModel.from_pretrained(embedding_config[\"model_name\"])\n",
        "    self.dropout=nn.Dropout(0.5)\n",
        "    self.classifier=nn.Linear(input_size, output_size)\n",
        "    nn.init.xavier_uniform_(self.classifier.weight)"
      ],
      "metadata": {
        "id": "ViBNZBIcBHGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(predicted):\n",
        "  opt_threshold=0.5\n",
        "  predicted = [threshold(pred, opt_threshold) for pred in \n",
        "              predicted]\n",
        "  return predicted\n",
        "\n",
        "def evaluate_predictions(predictions, labels):\n",
        "  metrics.confusion_matrix(labels, predictions)\n",
        "  print(metrics.classification_report(labels, predictions, digits=4))\n",
        "  return f1_score(labels, predictions, average='macro')\n",
        "\n",
        "def normalize_threshold(predicted, y_test):\n",
        "  #opt_threshold=-2.850132\n",
        "  opt_threshold = 0.5\n",
        "  #opt_threshold = roc_curve_threshold(predicted, y_test)\n",
        "  #opt_threshold = max_f1_threshold(model,predicted_prob, y_test, X_test)\n",
        "  opt_threshold = pr_curve_threshold(predicted, y_test)\n",
        "  #opt_threshold = pr_curve_threshold(model.predict(X_train), y_train)\n",
        "  # opt_threshold = opt_threshold_tuning(predicted_prob, y_test)\n",
        "  predicted = [threshold(pred, opt_threshold) for pred in \n",
        "              predicted]\n",
        "  return predicted"
      ],
      "metadata": {
        "id": "2iq5eyJjBJli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, x_test, y_test):\n",
        "  x_test = model.tokenizer(x_test, padding  = \"max_length\", max_length = embedding_config[\"max_length\"], truncation = True, return_tensors = \"pt\")\n",
        "  test_data = TensorDataset(x_test[\"input_ids\"], x_test[\"attention_mask\"], torch.FloatTensor(y_test))\n",
        "\n",
        "  batch_size = embedding_config[\"batch_size\"]\n",
        "  test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "  preds=[]\n",
        "  model.eval()\n",
        "  for batch in test_dataloader:\n",
        "    aux_batch = {\n",
        "        \"input_ids\" : batch[0].to(device),\n",
        "        \"attention_mask\" : batch[1].to(device)\n",
        "    }\n",
        "\n",
        "    with torch.no_grad():\n",
        "      outputs = model.embedder(**aux_batch).pooler_output\n",
        "      outputs = model.classifier(model.dropout(outputs))\n",
        "\n",
        "    preds+=outputs.sigmoid().round().reshape(-1).tolist()\n",
        "    #preds+=outputs.reshape(-1).tolist()\n",
        "  \n",
        "  # predictions = normalize(preds)\n",
        "  # predictions = normalize_threshold(preds, y_test)\n",
        "  return evaluate_predictions(preds, y_test)"
      ],
      "metadata": {
        "id": "1DmgskYqBLbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, x_train, y_train, x_valid, y_valid):\n",
        "  max_score = 0\n",
        "  \n",
        "  x_train = model.tokenizer(x_train, padding  = \"max_length\", max_length = embedding_config[\"max_length\"], truncation = True, return_tensors = \"pt\")\n",
        "  train_data = TensorDataset(x_train[\"input_ids\"], x_train[\"attention_mask\"], torch.FloatTensor(y_train))\n",
        "\n",
        "  batch_size = embedding_config[\"batch_size\"]\n",
        "  train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
        "\n",
        "  num_epochs = model_config[\"epochs\"]\n",
        "  num_training_steps = num_epochs * len(train_dataloader)\n",
        "\n",
        "  optimizer = AdamW(model.parameters(), lr = model_config[\"learning_rate\"], weight_decay = model_config[\"weight_decay\"])\n",
        "\n",
        "  lr_scheduler = get_scheduler(\n",
        "      \"linear\",\n",
        "      optimizer = optimizer,\n",
        "      num_warmup_steps = 0.2*num_training_steps,\n",
        "      num_training_steps = num_training_steps\n",
        "  )\n",
        "\n",
        "  print(\"Steps \",num_training_steps)\n",
        "\n",
        "  progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "  model.train()\n",
        "  losses = []\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    print(\"Epoch: \", epoch+1)\n",
        "    for batch in train_dataloader:\n",
        "      aux_batch = {\n",
        "        \"input_ids\" : batch[0].to(device),\n",
        "        \"attention_mask\" : batch[1].to(device)\n",
        "      }\n",
        "\n",
        "      outputs = model.embedder(**aux_batch).pooler_output\n",
        "      outputs = model.classifier(model.dropout(outputs))\n",
        "\n",
        "      criterion = nn.CrossEntropyLoss()#??????????\n",
        "      loss = criterion(outputs, batch[2].to(device).reshape(-1,1))\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "      lr_scheduler.step()\n",
        "      progress_bar.update(1)\n",
        "\n",
        "    losses.append(loss.tolist())\n",
        "    score =  evaluate(model, x_valid, y_valid)\n",
        "    if score>max_score:\n",
        "      max_score = score\n",
        "      #torch.save(model.state_dict(), embedding_config[\"path\"]+\".pt\")  \n",
        "  return num_training_steps, losses"
      ],
      "metadata": {
        "id": "LZZ2gadkBN-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertClassfierPytorch()\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "9pk5pMOABUWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_train = train_x.astype(str).tolist()\n",
        "text_test = test_x.astype(str).tolist()"
      ],
      "metadata": {
        "id": "Qw0VCg-pCbhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(train_x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISWmYZjyCtGc",
        "outputId": "1552b9b5-38a3-40a3-8e1e-28795cda8dcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.series.Series'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_training_steps, losses = train_model(model, text_train, train_y, text_test, test_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "589f7e245a894b829d19087da0fcae1c",
            "267537a8a87a456abd823ce59303f23b",
            "afb81b768b1c44e199dd739c2393a865",
            "ae578cf93f3a4b35b02be8bdbe696c89",
            "ca012c4121204b7fbb964f4a71224cda",
            "b932352b9655484093b519239800c724",
            "b1ae841ef67644828759f4c80d712102",
            "2b3f0a8541bf41d6afa35e3f0783d6df",
            "964d18ad8ebb48a3aec9b81b54cbdd51",
            "351ec7e793f5443a83867d104ae7ff52",
            "57263abdc4294a61b3a64b61c52e9d21"
          ]
        },
        "id": "G0Mcyj7LBXQV",
        "outputId": "00860e3d-1a60-4b2a-a12a-6f680e02a349"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Steps  10100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "589f7e245a894b829d19087da0fcae1c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5855    0.4299    0.4958      2198\n",
            "           1     0.3762    0.6091    0.4651      1522\n",
            "           2     0.0000    0.0000    0.0000       358\n",
            "\n",
            "    accuracy                         0.4590      4078\n",
            "   macro avg     0.3206    0.3463    0.3203      4078\n",
            "weighted avg     0.4560    0.4590    0.4408      4078\n",
            "\n",
            "Epoch:  2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5855    0.4299    0.4958      2198\n",
            "           1     0.3762    0.6091    0.4651      1522\n",
            "           2     0.0000    0.0000    0.0000       358\n",
            "\n",
            "    accuracy                         0.4590      4078\n",
            "   macro avg     0.3206    0.3463    0.3203      4078\n",
            "weighted avg     0.4560    0.4590    0.4408      4078\n",
            "\n",
            "Epoch:  3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5855    0.4299    0.4958      2198\n",
            "           1     0.3762    0.6091    0.4651      1522\n",
            "           2     0.0000    0.0000    0.0000       358\n",
            "\n",
            "    accuracy                         0.4590      4078\n",
            "   macro avg     0.3206    0.3463    0.3203      4078\n",
            "weighted avg     0.4560    0.4590    0.4408      4078\n",
            "\n",
            "Epoch:  4\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-dff09756e5d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnum_training_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-48-744c575e1f0e>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, x_train, y_train, x_valid, y_valid)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m       \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/optimization.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"params\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1102\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "code.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "IMHPbMBI4VTQ",
        "7MBdfMwk4bR_",
        "2abJM4PN4yyY",
        "5_goD7gM5Bnj",
        "KsTCPqZS4jzh",
        "cWK3dbEM45Fj",
        "Kz3D-3YoDine",
        "lgvLSOR8gsOn",
        "tUiQnNNV5Frg"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "589f7e245a894b829d19087da0fcae1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_267537a8a87a456abd823ce59303f23b",
              "IPY_MODEL_afb81b768b1c44e199dd739c2393a865",
              "IPY_MODEL_ae578cf93f3a4b35b02be8bdbe696c89"
            ],
            "layout": "IPY_MODEL_ca012c4121204b7fbb964f4a71224cda"
          }
        },
        "267537a8a87a456abd823ce59303f23b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b932352b9655484093b519239800c724",
            "placeholder": "​",
            "style": "IPY_MODEL_b1ae841ef67644828759f4c80d712102",
            "value": " 17%"
          }
        },
        "afb81b768b1c44e199dd739c2393a865": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b3f0a8541bf41d6afa35e3f0783d6df",
            "max": 10100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_964d18ad8ebb48a3aec9b81b54cbdd51",
            "value": 1764
          }
        },
        "ae578cf93f3a4b35b02be8bdbe696c89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_351ec7e793f5443a83867d104ae7ff52",
            "placeholder": "​",
            "style": "IPY_MODEL_57263abdc4294a61b3a64b61c52e9d21",
            "value": " 1764/10100 [04:39&lt;19:03,  7.29it/s]"
          }
        },
        "ca012c4121204b7fbb964f4a71224cda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b932352b9655484093b519239800c724": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1ae841ef67644828759f4c80d712102": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b3f0a8541bf41d6afa35e3f0783d6df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "964d18ad8ebb48a3aec9b81b54cbdd51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "351ec7e793f5443a83867d104ae7ff52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57263abdc4294a61b3a64b61c52e9d21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}