{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanduerhan/Licenta/blob/main/code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OH7f6H-lQfOA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Import"
      ],
      "metadata": {
        "id": "EOmssdxm3o5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "RI7LFohhh6oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade numpy"
      ],
      "metadata": {
        "id": "BHx29AkXpS9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "4DYONP6Xrwer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ZX3Ibl5BQUFC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import openpyxl\n",
        "from nltk.corpus import stopwords, words, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "import gensim\n",
        "import re, string\n",
        "from string import punctuation, digits\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import one_hot,Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.initializers import Constant\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import ReLU\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense , Flatten ,Embedding,Input,LSTM\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "col_list = [\"Column2\", \"Column3\"]\n",
        "dataset = pd.read_excel(\"Training.data.xlsx\", usecols=col_list)\n",
        "testset = pd.read_excel(\"Dev.xlsx\", usecols=col_list)\n",
        "\n",
        "\n",
        "def add_category_id(dataset):\n",
        "  dataset['category_id'] = dataset['Column3'].factorize()[0]\n",
        "  category_id_dataset = dataset[['Column3', 'category_id']].drop_duplicates()\n",
        "\n",
        "  category_to_id = dict(category_id_dataset.values)\n",
        "  id_to_category = dict(category_id_dataset[['category_id', 'Column3']].values)\n",
        "  return dataset,category_to_id, id_to_category\n",
        "\n",
        "dataset, category_to_id_train, id_to_category_train = add_category_id(dataset)\n",
        "testset, category_to_id_test, id_to_category_test = add_category_id(testset)\n",
        "dataset = dataset.sample(frac = 1)\n",
        "testset = testset.sample(frac = 1)\n",
        "train_x = dataset[\"Column2\"]\n",
        "train_y = dataset[\"category_id\"]\n",
        "test_x = testset[\"Column2\"]\n",
        "test_y = testset[\"category_id\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_x[7412])"
      ],
      "metadata": {
        "id": "Yy4EFKrVeZaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tweets = dataset['Column2'].tolist()\n",
        "test_tweets = testset['Column2'].tolist()\n",
        "def keep_uniques(array, df):\n",
        "    dels=[]\n",
        "    for i in array:\n",
        "        if array.count(i)>1:\n",
        "            dels.append(i)\n",
        "    dels=list(set(dels))\n",
        "    for i in dels:\n",
        "        df.drop( df[ df['Column2'] == i ].index, inplace=True)\n",
        "    return df\n",
        "\n",
        "dataset = keep_uniques(train_tweets, dataset)\n",
        "testset = keep_uniques(test_tweets, testset)\n",
        "\n",
        "print(len(testset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_jdBb8Z_Pus",
        "outputId": "4bdd86af-ceb3-45a2-bd83-5498c1b58d84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fig = plt.figure(figsize=(8,6))\n",
        "colors = ['lightblue','blue','darkblue']\n",
        "\n",
        "train_mean = dataset.groupby('Column3').Column2.count().sort_values()\n",
        "print(train_mean)\n",
        "test_mean = testset.groupby('Column3').Column2.count().sort_values()\n",
        "X = ['Antrenare','Validare']\n",
        "severe_mean = [train_mean[0], test_mean[0]]\n",
        "not_mean = [train_mean[1], test_mean[1]]\n",
        "moderate_mean = [train_mean[2], test_mean[2]]\n",
        "X_axis = np.arange(len(X))\n",
        "\n",
        "plt.bar(X_axis , moderate_mean, 0.2, label = 'Moderat',color =\"darkblue\")\n",
        "plt.bar(X_axis + 0.2, not_mean, 0.2, label = 'Fără depresie',color = 'blue')\n",
        "plt.bar(X_axis + 0.2*2, severe_mean, 0.2, label = 'Sever',color = 'lightblue')\n",
        "  \n",
        "plt.xticks(X_axis, X)\n",
        "plt.ylabel(\"Număr de apariții\")\n",
        "plt.title(\"Tweeturi per categorie\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PQ4_yB2Cdnad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = ['SMOTE', 'Ponderi de Clase', 'Nebalansat']\n",
        "vec1 = [0.4224,0.4297, 0.3807]\n",
        "vec2 = [0.4375,0.4505,0.4275]\n",
        "vec3 = [0.4793,0.4643, 0.4612]\n",
        "vec4 = [0.4776,0.4690, 0.4441]\n",
        "X_axis = np.arange(3)\n",
        "\n",
        "plt.bar(X_axis , vec4, 0.2, color =\"darkblue\")\n",
        "plt.bar(X_axis + 0.2, vec3, 0.2, color = 'blue')\n",
        "plt.bar(X_axis + 0.2*2, vec2, 0.2, color = 'lightblue')\n",
        "plt.bar(X_axis + 0.2*3, vec1, 0.2, color = 'blue')\n",
        "plt.xticks(X_axis, X)\n",
        "plt.ylabel(\"Scor F1\")\n",
        "plt.title(\"Metode de balansare a datelor\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C1vHNHDjr4SA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "e69001fc-2fb6-4cf7-99ab-3a24efa0644b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PreProcessing"
      ],
      "metadata": {
        "id": "xHx-Yr8O3pvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tweet-preprocessor\n",
        "import preprocessor as p\n",
        "\n",
        "def preprocess_tweets(text):\n",
        "  return p.clean(text)\n",
        "\n",
        "print(preprocess_tweets(train_x[7412]))"
      ],
      "metadata": {
        "id": "i5ORHpDKp2eO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3fdaf8b-f65a-43c1-aaf5-ff93cc4a5245"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tweet-preprocessor\n",
            "  Downloading tweet_preprocessor-0.6.0-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: tweet-preprocessor\n",
            "Successfully installed tweet-preprocessor-0.6.0\n",
            "Today i want to kill myself.... : Because i'm i alone and no one loves me anymore :'(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# cleaning the text : deleting the emojis, the existing emails, the punctuation, the present digits,\n",
        "# the hyperlinks and the stopwords (a,the,is, etc)\n",
        "\n",
        "def delete_emoji(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               u\"\\U0001f926-\\U0001f937\"\n",
        "                               u\"\\U00010000-\\U0010ffff\"\n",
        "                               u\"\\u2640-\\u2642\"\n",
        "                               u\"\\u2600-\\u2B55\"\n",
        "                               u\"\\u200d\"\n",
        "                               u\"\\u23cf\"\n",
        "                               u\"\\u23e9\"\n",
        "                               u\"\\u231a\"\n",
        "                               u\"\\ufe0f\"  # dingbats\n",
        "                               u\"\\u3030\"\n",
        "                               \"]+\", re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)  # no emoji\n",
        "\n",
        "\n",
        "def delete_digits(text):\n",
        "    text = text.lower()\n",
        "    clean = text.translate(str.maketrans('', '', digits))\n",
        "    return clean\n",
        "\n",
        "\n",
        "def delete_punctuation(text):\n",
        "    clean = text.translate(str.maketrans('', '', punctuation + '’“”'))\n",
        "    return clean\n",
        "\n",
        "\n",
        "stop = set(stopwords.words('english'))\n",
        "stop.add('gtpoplt')\n",
        "stop.add('new')\n",
        "stop.add('year')\n",
        "stop.add('eve')\n",
        "stop.add('years')\n",
        "stop.add('ti')\n",
        "stop.add('ame')\n",
        "stop.add('folks')\n",
        "punct = list(string.punctuation)\n",
        "stop.update(punct)\n",
        "\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    final_text = []\n",
        "    for i in text.split():\n",
        "        if i.strip().lower() not in stop:\n",
        "            final_text.append(i.strip())\n",
        "    return \" \".join(final_text)\n",
        "\n",
        "\n",
        "def remove_hyperlinks(text):\n",
        "    for word in text:\n",
        "        if re.match(r'^http', word):\n",
        "            text.remove(word)\n",
        "    return \" \".join(text)\n",
        "\n",
        "def remove_emails(text):\n",
        "    text = text.split()\n",
        "    for i in text:\n",
        "        if '@' in i.strip().lower():\n",
        "            text.remove(i)\n",
        "    return \" \".join(text)\n",
        "\n",
        "contraction_dict = {\"youre\":\"you are\",\"im\": \"i am\",\"wouldnt\": \"would not\",\"itll\": \"it will\",\"wasnt\": \"was not\",\"dont\": \"do not\",\"ill\": \"i will\",\"isnt\": \"is not\",\"cant\": \"cannot\",\"arent\": \"are not\",\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
        "def _get_contractions(contraction_dict):\n",
        "    contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n",
        "    return contraction_dict, contraction_re\n",
        "contractions, contractions_re = _get_contractions(contraction_dict)\n",
        "def replace_contractions(text):\n",
        "    def replace(match):\n",
        "        return contractions[match.group(0)]\n",
        "    return contractions_re.sub(replace, text)\n",
        "\n",
        "# def denoise_text(text):\n",
        "#     text = replace_contractions(text)\n",
        "#     text = remove_words(text)\n",
        "#     text = remove_emails(text)\n",
        "#     text = delete_punctuation(text)\n",
        "#     text = delete_emoji(text)\n",
        "#     text = delete_digits(text)\n",
        "#     text = remove_stopwords(text)\n",
        "#     text = text.split()\n",
        "#     text = remove_hyperlinks(text)\n",
        "#     return text\n",
        "\n",
        "\n",
        "\n",
        "train_x = dataset[\"Column2\"]\n",
        "train_y = dataset[\"category_id\"]\n",
        "test_x = testset[\"Column2\"]\n",
        "test_y = testset[\"category_id\"]\n",
        "\n",
        "for i in range(len(train_x)):\n",
        "  train_x[i] = replace_contractions(train_x[i])\n",
        "  train_x[i] = remove_emails(train_x[i])\n",
        "  train_x[i] = delete_punctuation(train_x[i])\n",
        "  train_x[i] = delete_emoji(train_x[i])\n",
        "  train_x[i] = delete_digits(train_x[i])\n",
        "  train_x[i] = remove_stopwords(train_x[i])\n",
        "\n",
        "for i in range(len(test_x)):\n",
        "  test_x[i] = replace_contractions(test_x[i])\n",
        "  test_x[i] = remove_emails(test_x[i])\n",
        "  test_x[i] = delete_punctuation(test_x[i])\n",
        "  test_x[i] = delete_emoji(test_x[i])\n",
        "  test_x[i] = delete_digits(test_x[i])\n",
        "  test_x[i] = remove_stopwords(test_x[i])\n",
        "\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "fySiVht3QUFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_x[7449])\n",
        "print(dataset[\"Column2\"])"
      ],
      "metadata": {
        "id": "RfI378zkgRMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatization"
      ],
      "metadata": {
        "id": "cfS5G2jQ4GGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayod5PmNYU7H",
        "outputId": "eb65307a-4e4a-4f2c-f0a3-1366388466d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/omw-1.4.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# lemmatization : crying -> cry, days -> day\n",
        "\n",
        "def get_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "\n",
        "def lemm(text):\n",
        "    text = text.split()\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    sar_list_lemmatizer = [lemmatizer.lemmatize(word, get_pos(word)) for word in text]\n",
        "    return \" \".join(sar_list_lemmatizer)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "3oSwfDyzQUFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = train_x.apply(lemm)\n",
        "test_x = test_x.apply(lemm)"
      ],
      "metadata": {
        "id": "Zv21BaGJBhgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_x[7429])"
      ],
      "metadata": {
        "id": "0Gst2fP9GIvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "text = ' '.join(train_x[dataset['Column3']=='severe'])\n",
        "wc = WordCloud(background_color='white').generate(text)\n",
        "plt.imshow(wc)"
      ],
      "metadata": {
        "id": "t0Oao20MI9Mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Count Vectorizer"
      ],
      "metadata": {
        "id": "IMHPbMBI4VTQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8068, 10252)\n"
          ]
        }
      ],
      "source": [
        "# CountVectorizer reprezentation for the user tweets\n",
        "def count_vec(train,test):\n",
        "  vectorizer = CountVectorizer()\n",
        "  x_train_cv = vectorizer.fit_transform(train)\n",
        "\n",
        "  x_test_cv = vectorizer.transform(test)\n",
        "  return x_train_cv, x_test_cv"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "8blt8KSfQUFd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4698c629-c256-4e82-dfc5-bdedd44d5259"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_cv, x_test_cv = count_vec(train_x, test_x)"
      ],
      "metadata": {
        "id": "XJK62wRaBvyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data imbalance handling"
      ],
      "metadata": {
        "id": "7MBdfMwk4bR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sm = SMOTE(random_state = 42)\n",
        "res_x, res_y = sm.fit_resample(x_train_cv, train_y)\n",
        "print(\"After OverSampling, counts of label '2': {}\".format(sum(res_y == 2)))"
      ],
      "metadata": {
        "id": "w-x13nL-a0EM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ce72343-0a4b-44c1-b08d-14ec876634bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After OverSampling, counts of label '2': 5647\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os = RandomOverSampler()\n",
        "res_x2, res_y2 = os.fit_resample(x_train_cv, train_y)"
      ],
      "metadata": {
        "id": "020ZvbDV8Yk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tf-Idf "
      ],
      "metadata": {
        "id": "2abJM4PN4yyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tf-idf reprezentation for the tweets\n",
        "def tf_idf(train, test, y):\n",
        "  vectorizer = TfidfVectorizer(max_df = 0.15,min_df = 5, ngram_range=(1,2), stop_words='english')\n",
        "  train_x_tf = vectorizer.fit_transform(train)\n",
        "  test_x_tf = vectorizer.transform(test)\n",
        "\n",
        "  res_tfx, res_tfy = sm.fit_resample(train_x_tf, y)\n",
        "  return train_x_tf, test_x_tf, res_tfx, res_tfy"
      ],
      "metadata": {
        "id": "WkoI7tMeRnXz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d092a0ca-aae1-4811-ea8c-9141d06ccb71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8068, 33909)\n",
            "(16941, 33909)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_x_tf, test_x_tf, res_tfx, res_tfy = tf_idf(train_x, test_x, train_y)"
      ],
      "metadata": {
        "id": "AkqK0TxgB_k9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word2Vec"
      ],
      "metadata": {
        "id": "5_goD7gM5Bnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent = [row.split() for row in train_x]"
      ],
      "metadata": {
        "id": "Q_TNLIZYk8DA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.phrases import Phrases, Phraser"
      ],
      "metadata": {
        "id": "wH95fWT2lHE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram = Phraser(phrases)\n",
        "sentences = bigram[sent]"
      ],
      "metadata": {
        "id": "ffE5uhQflNF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentences)"
      ],
      "metadata": {
        "id": "xslKOh8bnezF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model = Word2Vec(min_count=20,\n",
        "                     window=5,\n",
        "                     size=300,\n",
        "                     sample=6e-5, \n",
        "                     alpha=0.03, \n",
        "                     min_alpha=0.0007, \n",
        "                     negative=20,\n",
        "                     )"
      ],
      "metadata": {
        "id": "XP-cfBanlg4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.build_vocab(sentences)"
      ],
      "metadata": {
        "id": "FjD343pblz0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)"
      ],
      "metadata": {
        "id": "uHmC66qBl6T1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.wv.most_similar(positive=[\"depression\"])"
      ],
      "metadata": {
        "id": "rSI6JFcmmM43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model,word_index,pad_rev_train = word2(train_x)"
      ],
      "metadata": {
        "id": "eJRUq5f6v1FO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2,word_index2,pad_rev_test = word2(test_x)"
      ],
      "metadata": {
        "id": "kwKmyoLL0HF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_weight_matrix(model, vocab):\n",
        "    vocab_size = len(vocab) + 1\n",
        "    weight_matrix = np.zeros((vocab_size, 300))\n",
        "    for word, i in vocab.items():\n",
        "        weight_matrix[i] = model[word]\n",
        "    return weight_matrix"
      ],
      "metadata": {
        "id": "O25ga7HBxIk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_vectors = get_weight_matrix(model, word_index)"
      ],
      "metadata": {
        "id": "XBosyRN8xMkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def w2v_embedding(train_text):\n",
        "\n",
        "  lst_corpus = []\n",
        "  for string in train_text:\n",
        "    lst_words = string.split()\n",
        "    lst_grams = [\" \".join(lst_words[i:i+1]) \n",
        "                for i in range(0, len(lst_words), 1)]\n",
        "    lst_corpus.append(lst_grams)\n",
        "\n",
        "  bigrams_detector = gensim.models.phrases.Phrases(lst_corpus, \n",
        "                  delimiter=\" \".encode(), min_count=30)\n",
        "  bigrams_detector = gensim.models.phrases.Phraser(bigrams_detector)\n",
        "  trigrams_detector = gensim.models.phrases.Phrases(bigrams_detector[lst_corpus], \n",
        "              delimiter=\" \".encode(), min_count=30)\n",
        "  trigrams_detector = gensim.models.phrases.Phraser(trigrams_detector)\n",
        "\n",
        "  nlp = Word2Vec(lst_corpus, size=300,   \n",
        "            window=8, min_count=2, sg=1, iter=30)\n",
        "  return nlp, lst_corpus, bigrams_detector, trigrams_detector"
      ],
      "metadata": {
        "id": "YqW8Yiqr4vEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_engineering(lst_corpus, train_text):\n",
        "  tokenizer = Tokenizer(lower=True, split=' ', \n",
        "                     oov_token=\"NaN\", \n",
        "                     filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
        "  tokenizer.fit_on_texts(lst_corpus)\n",
        "  dic_vocabulary = tokenizer.word_index\n",
        "\n",
        "  lst_corpus = []\n",
        "  for string in train_text:\n",
        "    lst_words = string.split()\n",
        "    lst_grams = [\" \".join(lst_words[i:i+1]) \n",
        "                for i in range(0, len(lst_words), 1)]\n",
        "    lst_corpus.append(lst_grams)\n",
        "  lst_text2seq= tokenizer.texts_to_sequences(lst_corpus)\n",
        "\n",
        "  X_train = pad_sequences(lst_text2seq, \n",
        "                      maxlen=30, padding=\"post\", truncating=\"post\")\n",
        "  return tokenizer, dic_vocabulary, X_train"
      ],
      "metadata": {
        "id": "HXONvgmg40qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_embedding_matrix(dic_vocabulary, nlp):\n",
        "  embeddings = np.zeros((len(dic_vocabulary)+1, 300))\n",
        "  for word,idx in dic_vocabulary.items():\n",
        "      try:\n",
        "          embeddings[idx] =  nlp[word]\n",
        "      except:\n",
        "          pass\n",
        "  return embeddings"
      ],
      "metadata": {
        "id": "w3XdYm2A421r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp, lst_corpus, bigrams_detector, trigrams_detector = w2v_embedding(train_x)"
      ],
      "metadata": {
        "id": "eYbWGZel4488"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(nlp.wv.most_similar(positive=\"depression\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WIjrfeL9fYT",
        "outputId": "dcfc0b8a-29a7-448d-93e7-30ec3ea94d53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('ocdanxiety', 0.3492339253425598), ('anxiety', 0.3416626453399658), ('diagnose', 0.33797287940979004), ('longstanding', 0.3372364044189453), ('actor', 0.33621853590011597), ('stabilizer', 0.334298312664032), ('crucial', 0.33266133069992065), ('microdosing', 0.3280254006385803), ('selenium', 0.32584166526794434), ('sought', 0.32460087537765503)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer, dic_vocabulary, X_train = feature_engineering(lst_corpus,train_x)"
      ],
      "metadata": {
        "id": "4O1vFy469Jaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYBf8fPNBzrc",
        "outputId": "16984993-2e68-42e3-cdc1-6b5c6395e7fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8068, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_train = make_embedding_matrix(dic_vocabulary, nlp)"
      ],
      "metadata": {
        "id": "l3JJiPOE9QLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_handling(test_text,bigrams_detector, trigrams_detector, tokenizer):\n",
        "\n",
        "  lst_corpus = []\n",
        "  for string in test_text:\n",
        "      lst_words = string.split()\n",
        "      lst_grams = [\" \".join(lst_words[i:i+1]) for i in range(0, \n",
        "                  len(lst_words), 1)]\n",
        "      lst_corpus.append(lst_grams)\n",
        "\n",
        "  lst_corpus = list(bigrams_detector[lst_corpus])\n",
        "  lst_corpus = list(trigrams_detector[lst_corpus])\n",
        "\n",
        "  lst_text2seq = tokenizer.texts_to_sequences(lst_corpus)\n",
        "\n",
        "  X_test = pad_sequences(lst_text2seq, maxlen=30,\n",
        "              padding=\"post\", truncating=\"post\")\n",
        "  return X_test"
      ],
      "metadata": {
        "id": "iH5BAwFM_F8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp2, lst_corpus2, bigrams_detector2, trigrams_detector2 = w2v_embedding(test_x)"
      ],
      "metadata": {
        "id": "JGH7zCw3C3mA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = test_handling(test_x, bigrams_detector2, trigrams_detector2, tokenizer)"
      ],
      "metadata": {
        "id": "P7S9fXX7-E3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IH1zz40QCuUi",
        "outputId": "a974d23a-a64f-433d-dc6f-ff15ae4432ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4078, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes"
      ],
      "metadata": {
        "id": "KsTCPqZS4jzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def naive_bayes(train_x, train_y, test_x, test_y):\n",
        "  naive_bayes_classifier = MultinomialNB()\n",
        "  naive_bayes_classifier.fit(train_x, train_y)\n",
        "  pred_y = naive_bayes_classifier.predict(test_x)\n",
        "\n",
        "  score1 = metrics.accuracy_score(test_y, pred_y)\n",
        "  print(\"Accuracy \" + str(score1))\n",
        "  print(metrics.classification_report(test_y, pred_y, digits = 4))"
      ],
      "metadata": {
        "id": "uDe2Ts7ZgnZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "naive_bayes(res_x, res_y, x_test_cv, test_y)"
      ],
      "metadata": {
        "id": "o4k1tyGnC79-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "naive_bayes(x_train_cv, train_y, x_test_cv, test_y)"
      ],
      "metadata": {
        "id": "5KhMSw_4DQi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "naive_bayes(res_tfx, res_tfy, test_x_tf, test_y)"
      ],
      "metadata": {
        "id": "bLuvDkoGDBGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "naive_bayes(res_tfx, res_tfy, test_x_tf, test_y)"
      ],
      "metadata": {
        "id": "n8bhAxIODG7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AdaBoost"
      ],
      "metadata": {
        "id": "cWK3dbEM45Fj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def adaboost(train_x, train_y, test_x, test_y):\n",
        "  ada = AdaBoostClassifier()\n",
        "  boost = ada.fit(train_x, train_y)\n",
        "  pred_y = boost.predict(test_x)\n",
        "  print(\"AdaBoost Classifier Model Accuracy:\", accuracy_score(test_y, pred_y))\n",
        "  print(metrics.classification_report(test_y, pred_y))"
      ],
      "metadata": {
        "id": "6QDGISapwPVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adaboost(res_x, res_y, x_test_cv, test_y)"
      ],
      "metadata": {
        "id": "tI9SyLLFDopm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adaboost(x_train_cv, train_y, x_test_cv, test_y)"
      ],
      "metadata": {
        "id": "_WdD3RTFDptd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adaboost(res_tfx, res_tfy, test_x_tf, test_y)"
      ],
      "metadata": {
        "id": "7AXgolmBDp5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adaboost(res_tfx, res_tfy, test_x_tf, test_y)"
      ],
      "metadata": {
        "id": "1Exgj1HjDp-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression"
      ],
      "metadata": {
        "id": "Kz3D-3YoDine"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_weight = {0: 1.,\n",
        "                1: 3.,\n",
        "                2: 4.}\n",
        "\n",
        "def logistic_regression(train_x, train_y, test_x, test_y):\n",
        "  model = LogisticRegression(multi_class='multinomial', solver='newton-cg')\n",
        "\n",
        "  model = model.fit(train_x, train_y)\n",
        "  y_pred = model.predict(test_x)\n",
        "\n",
        "  print(\"Regression Classifier Model Accuracy:\", accuracy_score(test_y, y_pred))\n",
        "  print(metrics.classification_report(test_y, y_pred, digits = 4))\n",
        "\n",
        "  conf_mat = confusion_matrix(test_y, y_pred)\n",
        "  print(conf_mat)\n"
      ],
      "metadata": {
        "id": "hKYzd_kPDmDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_regression(res_x, res_y, x_test_cv, test_y)"
      ],
      "metadata": {
        "id": "ObN_RomsEE_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_regression(x_train_cv, train_y, x_test_cv, test_y)"
      ],
      "metadata": {
        "id": "tHnCLO2AEGwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_regression(res_tfx, res_tfy, test_x_tf, test_y)"
      ],
      "metadata": {
        "id": "oS3nHbzAEG0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_regression(res_tfx, res_tfy, test_x_tf, test_y)"
      ],
      "metadata": {
        "id": "JdqY8HsYEG6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVM"
      ],
      "metadata": {
        "id": "6a-WtjG_Eilc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class_weight = {0: 1.,\n",
        "                1: 3.,\n",
        "                2: 6.}\n",
        "\n",
        "def svm(train_x, train_y, test_x, test_y):\n",
        "  model = LinearSVC()\n",
        "  model = model.fit(train_x_tf, train_y)\n",
        "  y_pred = model.predict(test_x_tf)\n",
        "  \n",
        "  print(\"Svc Classifier Model Accuracy:\", accuracy_score(test_y, y_pred))\n",
        "  print(metrics.classification_report(test_y, y_pred, digits = 4))\n",
        "\n",
        "  conf_mat = confusion_matrix(test_y, y_pred)\n",
        "  print(conf_mat)\n"
      ],
      "metadata": {
        "id": "wDRy1f9pv-I1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm(res_x, res_y, x_test_cv, test_y)"
      ],
      "metadata": {
        "id": "5-1sZW-mEaNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm(x_train_cv, train_y, x_test_cv, test_y)"
      ],
      "metadata": {
        "id": "kbrYJSE8Eaqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm(res_tfx, res_tfy, test_x_tf, test_y)"
      ],
      "metadata": {
        "id": "MSAahaAvEat0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm(res_tfx, res_tfy, test_x_tf, test_y)"
      ],
      "metadata": {
        "id": "gdlc9DW_Eazb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# F1 metric"
      ],
      "metadata": {
        "id": "lgvLSOR8gsOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def f1(y_true, y_pred):\n",
        "    y_pred = K.round(y_pred)\n",
        "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
        "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
        "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
        "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
        "\n",
        "    p = tp / (tp + fp + K.epsilon())\n",
        "    r = tp / (tp + fn + K.epsilon())\n",
        "\n",
        "    f1 = 2*p*r / (p+r+K.epsilon())\n",
        "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
        "    return K.mean(f1)"
      ],
      "metadata": {
        "id": "bdQgAL5I6cCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f1_loss(y_true, y_pred):\n",
        "    \n",
        "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
        "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
        "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
        "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
        "\n",
        "    p = tp / (tp + fp + K.epsilon())\n",
        "    r = tp / (tp + fn + K.epsilon())\n",
        "\n",
        "    f1 = 2*p*r / (p+r+K.epsilon())\n",
        "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
        "    return 1 - K.mean(f1)"
      ],
      "metadata": {
        "id": "p3koGLPkDxnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network"
      ],
      "metadata": {
        "id": "tUiQnNNV5Frg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_weight = {0: 1.,\n",
        "                1: 3.,\n",
        "                2: 5.}\n",
        "# def split(pad_rev,train_y):\n",
        "#   Y=keras.utils.to_categorical(train_y)  # one hot target as required by NN.\n",
        "#   x_train,x_test,y_train,y_test=train_test_split(pad_rev,Y,test_size=0.05,random_state=42)\n",
        "#   return x_train, x_test,y_train,y_test\n",
        "def network(embeddings):\n",
        " ## input\n",
        "  x_in = layers.Input(shape=(30,))\n",
        "  ## embedding\n",
        "  x = layers.Embedding(input_dim=embeddings.shape[0],  \n",
        "                      output_dim=embeddings.shape[1], \n",
        "                      weights=[embeddings],\n",
        "                      input_length=30, trainable=False)(x_in)\n",
        "  ## 2 layers of bidirectional lstm\n",
        "  x = layers.Bidirectional(layers.LSTM(units=30, dropout=0.2, \n",
        "                          return_sequences=True))(x)\n",
        "  x = layers.Bidirectional(layers.LSTM(units=30, dropout=0.2))(x)\n",
        "  ## final dense layers\n",
        "  x = layers.Dense(64, activation='elu')(x)\n",
        "  y_out = layers.Dense(3, activation='softmax')(x)\n",
        "  ## compile\n",
        "  model = Model(x_in, y_out)\n",
        "  model.compile(loss=f1_loss,\n",
        "                optimizer='adam', metrics=[f1])\n",
        "  return model\n",
        "Y=keras.utils.to_categorical(train_y)\n",
        "Y2=keras.utils.to_categorical(test_y)\n",
        "# x_train,x_test,y_train,y_test = split(X_train,train_y)\n",
        "model = network(embeddings_train)\n",
        "# x_valid, x_x, y_valid, y_y = split(x_test,test_y)\n",
        "model.fit(X_train, Y, epochs=10, batch_size=64, validation_data=(x_test,Y2))\n"
      ],
      "metadata": {
        "id": "QAeWwiGyMJFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "Z922lilzZ5KH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_y = model.predict(x_test)\n",
        "pred_y = np.argmax(pred_y, axis=1)\n",
        "# y_test=np.argmax(y_test, axis=1)\n",
        "print(metrics.classification_report(test_y, pred_y, digits = 4))\n",
        "\n",
        "conf_mat = confusion_matrix(test_y, pred_y)\n",
        "print(conf_mat)"
      ],
      "metadata": {
        "id": "tHB9SVT5O4On"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT"
      ],
      "metadata": {
        "id": "fccPGYtiwkXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "hdP6B25p2WM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "OOyweo6YFNFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.initializers import GlorotNormal\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "gr1JKdNUF9yM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizer"
      ],
      "metadata": {
        "id": "n_LErOdPgPJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
      ],
      "metadata": {
        "id": "ptBxQVHFgah4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_train = train_x.astype(str).tolist()\n",
        "text_test = test_x.astype(str).tolist()"
      ],
      "metadata": {
        "id": "PsYlNr3RgtrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = tokenizer.tokenize(train_x)\n",
        "embt = tokenizer.encode(s)"
      ],
      "metadata": {
        "id": "A7u9Cgbygh0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizerFast, RobertaForSequenceClassification,Trainer, TrainingArguments"
      ],
      "metadata": {
        "id": "V7Ld9eDDhqt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RobertaForSequenceClassification.from_pretrained('roberta-base')\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base', max_length = 512)"
      ],
      "metadata": {
        "id": "XuvlQftshfwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "QB_CWjoLjM6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas"
      ],
      "metadata": {
        "id": "fbAPquW4kHt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install load_dataframe"
      ],
      "metadata": {
        "id": "aVdi83dlkCWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "rllg8s-BkiQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer(\"all-mpnet-base-v2\")"
      ],
      "metadata": {
        "id": "cz2XLEkFgy2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence(texts):\n",
        "  model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
        "  embeddings = model.encode(texts)\n",
        "\n",
        "  return embeddings"
      ],
      "metadata": {
        "id": "IBHUesv8FbMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRANSFORMER_BATCH=128\n",
        "\n",
        "def count_embedd (df):\n",
        "    idx_chunk=list(df.columns).index('Column2')\n",
        "    embedd_lst = []\n",
        "    for index in range (0, len(df), TRANSFORMER_BATCH):\n",
        "        embedds = model.encode(df.iloc[index:index+TRANSFORMER_BATCH, idx_chunk].values, show_progress_bar=False)\n",
        "        embedd_lst.append(embedds)\n",
        "    return np.concatenate(embedd_lst)"
      ],
      "metadata": {
        "id": "HIZk6hn2aiu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_emb = count_embedd(dataset)"
      ],
      "metadata": {
        "id": "jyItDXSMgnog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_emb = count_embedd(testset)"
      ],
      "metadata": {
        "id": "-4U0N7MSg4Wr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.array(train_emb)\n",
        "X_test = np.array(test_emb)"
      ],
      "metadata": {
        "id": "CCkIOGTrbp2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train=keras.utils.to_categorical(train_y)"
      ],
      "metadata": {
        "id": "tf8ht-tgb_n_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KERAS_VALIDATION_SPLIT=0.00\n",
        "KERAS_EPOCHS=10\n",
        "KERAS_BATCH_SIZE=128\n",
        "\n",
        "class_weight = {0: 1.,\n",
        "                1: 3.,\n",
        "                2: 6.}\n",
        "\n",
        "# Create and train Keras model\n",
        "n_features=X_train.shape[1]\n",
        "n_labels = y_train.shape[1]\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Dense(2048, input_dim=n_features),\n",
        "  tf.keras.layers.LeakyReLU(alpha=0.01),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(64),\n",
        "  tf.keras.layers.LeakyReLU(alpha=0.01),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "LR=5e-4\n",
        "adam = keras.optimizers.Adam(learning_rate=LR, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "\n",
        "model.compile(optimizer=adam, \n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[f1])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=KERAS_EPOCHS, batch_size=KERAS_BATCH_SIZE, validation_split=KERAS_VALIDATION_SPLIT)"
      ],
      "metadata": {
        "id": "HiKl8FLGb3ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()\n"
      ],
      "metadata": {
        "id": "ZZ2HfzDYboIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_preds = model.predict(X_test)\n",
        "predicted = np.argmax(y_preds,axis=1) \n",
        "accuracy = metrics.accuracy_score(test_y, predicted)\n",
        "print(\"Accuracy:\",  round(accuracy,4))\n",
        "print(metrics.classification_report(test_y, predicted,digits=4))\n",
        "\n",
        "conf_mat = confusion_matrix(test_y, predicted)\n",
        "print(conf_mat)"
      ],
      "metadata": {
        "id": "vyOx-aVqcLte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT2"
      ],
      "metadata": {
        "id": "bYvS_CNvA0qU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzBSzy1OIDUK",
        "outputId": "c020e04a-4d09-4dcb-acbc-06e11e1b85ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/content/drive\")\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "2aLjBQPKIFVg",
        "outputId": "54837b04-e9c5-4c71-ac25-cb5e84fa822b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-44c501624541>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/content/drive\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ls'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/content/drive'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from transformers import AutoModel, AdamW, AutoTokenizer, get_scheduler\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset, RandomSampler\n",
        "import torch\n",
        "from sklearn.utils import class_weight\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import f1_score"
      ],
      "metadata": {
        "id": "iitfB3WWA4mJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_config = {\n",
        "    \"model_name\": \"cardiffnlp/twitter-roberta-base-sentiment\",\n",
        "    # \"path\":\"/content/drive/MyDrive/content/drive/models/twitter-roberta-base-sentiment-new\",\n",
        "    \"max_length\":40,\n",
        "    \"batch_size\":16,\n",
        "    \"source\":\"HuggingFace\"\n",
        "}"
      ],
      "metadata": {
        "id": "kYbWpL9gA6Ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_config = {\n",
        "    \"learning_rate\" : 2e-5,\n",
        "    \"weight_decay\":0.01,\n",
        "    \"epochs\":20\n",
        "}"
      ],
      "metadata": {
        "id": "PCm-p3_zA74b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "8MlTN7XeA9GP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights = class_weight.compute_class_weight(class_weight=\"balanced\", classes = np.unique(dataset[\"Column3\"]), y=np.array(dataset[\"Column3\"]))\n",
        "print(class_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDWyhE_XA-k3",
        "outputId": "1c62fba3-9ed3-4531-d00a-b7df396173f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.47624107 1.71842386 3.14174455]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BertClassfierPytorch(nn.Module):\n",
        "  def __init__(self, input_size = 768, output_size = 3):\n",
        "    super().__init__()\n",
        "    print(input_size)\n",
        "    self.name = embedding_config[\"model_name\"]\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(embedding_config[\"model_name\"], use_fast=False)\n",
        "    self.embedder=AutoModel.from_pretrained(embedding_config[\"model_name\"])\n",
        "    self.dropout=nn.Dropout(0.5)\n",
        "    self.classifier=nn.Linear(input_size, output_size)\n",
        "    nn.init.xavier_uniform_(self.classifier.weight)"
      ],
      "metadata": {
        "id": "ViBNZBIcBHGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(predicted):\n",
        "  opt_threshold=0.5\n",
        "  predicted = [threshold(pred, opt_threshold) for pred in \n",
        "              predicted]\n",
        "  return predicted\n",
        "\n",
        "def evaluate_predictions(predictions, labels):\n",
        "  metrics.confusion_matrix(labels, predictions)\n",
        "  print(metrics.classification_report(labels, predictions, digits=4))\n",
        "  return f1_score(labels, predictions, average='macro')\n",
        "\n",
        "def normalize_threshold(predicted, y_test):\n",
        "  #opt_threshold=-2.850132\n",
        "  opt_threshold = 0.5\n",
        "  #opt_threshold = roc_curve_threshold(predicted, y_test)\n",
        "  #opt_threshold = max_f1_threshold(model,predicted_prob, y_test, X_test)\n",
        "  opt_threshold = pr_curve_threshold(predicted, y_test)\n",
        "  #opt_threshold = pr_curve_threshold(model.predict(X_train), y_train)\n",
        "  # opt_threshold = opt_threshold_tuning(predicted_prob, y_test)\n",
        "  predicted = [threshold(pred, opt_threshold) for pred in \n",
        "              predicted]\n",
        "  return predicted"
      ],
      "metadata": {
        "id": "2iq5eyJjBJli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, x_test, y_test):\n",
        "  x_test = model.tokenizer(x_test, padding  = \"max_length\", max_length = embedding_config[\"max_length\"], truncation = True, return_tensors = \"pt\")\n",
        "  test_data = TensorDataset(x_test[\"input_ids\"], x_test[\"attention_mask\"], torch.FloatTensor(y_test))\n",
        "\n",
        "  batch_size = embedding_config[\"batch_size\"]\n",
        "  test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "  preds=[]\n",
        "  model.eval()\n",
        "  for batch in test_dataloader:\n",
        "    aux_batch = {\n",
        "        \"input_ids\" : batch[0].to(device),\n",
        "        \"attention_mask\" : batch[1].to(device)\n",
        "    }\n",
        "\n",
        "    with torch.no_grad():\n",
        "      outputs = model.embedder(**aux_batch).pooler_output\n",
        "      outputs = model.classifier(model.dropout(outputs))\n",
        "\n",
        "    preds+=outputs.sigmoid().round().reshape(-1).tolist()\n",
        "    #preds+=outputs.reshape(-1).tolist()\n",
        "  \n",
        "  # predictions = normalize(preds)\n",
        "  # predictions = normalize_threshold(preds, y_test)\n",
        "  return evaluate_predictions(preds, y_test)"
      ],
      "metadata": {
        "id": "1DmgskYqBLbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, x_train, y_train, x_valid, y_valid):\n",
        "  max_score = 0\n",
        "  \n",
        "  x_train = model.tokenizer(x_train, padding  = \"max_length\", max_length = embedding_config[\"max_length\"], truncation = True, return_tensors = \"pt\")\n",
        "  train_data = TensorDataset(x_train[\"input_ids\"], x_train[\"attention_mask\"], torch.FloatTensor(y_train))\n",
        "\n",
        "  batch_size = embedding_config[\"batch_size\"]\n",
        "  train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
        "\n",
        "  num_epochs = model_config[\"epochs\"]\n",
        "  num_training_steps = num_epochs * len(train_dataloader)\n",
        "\n",
        "  optimizer = AdamW(model.parameters(), lr = model_config[\"learning_rate\"], weight_decay = model_config[\"weight_decay\"])\n",
        "\n",
        "  lr_scheduler = get_scheduler(\n",
        "      \"linear\",\n",
        "      optimizer = optimizer,\n",
        "      num_warmup_steps = 0.2*num_training_steps,\n",
        "      num_training_steps = num_training_steps\n",
        "  )\n",
        "\n",
        "  print(\"Steps \",num_training_steps)\n",
        "\n",
        "  progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "  model.train()\n",
        "  losses = []\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    print(\"Epoch: \", epoch+1)\n",
        "    for batch in train_dataloader:\n",
        "      aux_batch = {\n",
        "        \"input_ids\" : batch[0].to(device),\n",
        "        \"attention_mask\" : batch[1].to(device)\n",
        "      }\n",
        "\n",
        "      outputs = model.embedder(**aux_batch).pooler_output\n",
        "      outputs = model.classifier(model.dropout(outputs))\n",
        "\n",
        "      criterion = nn.CrossEntropyLoss()#??????????\n",
        "      loss = criterion(outputs, batch[2].to(device).reshape(-1,1))\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "      lr_scheduler.step()\n",
        "      progress_bar.update(1)\n",
        "\n",
        "    losses.append(loss.tolist())\n",
        "    score =  evaluate(model, x_valid, y_valid)\n",
        "    if score>max_score:\n",
        "      max_score = score\n",
        "      #torch.save(model.state_dict(), embedding_config[\"path\"]+\".pt\")  \n",
        "  return num_training_steps, losses"
      ],
      "metadata": {
        "id": "LZZ2gadkBN-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertClassfierPytorch()\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "9pk5pMOABUWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_train = train_x.astype(str).tolist()\n",
        "text_test = test_x.astype(str).tolist()"
      ],
      "metadata": {
        "id": "Qw0VCg-pCbhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(train_x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISWmYZjyCtGc",
        "outputId": "1552b9b5-38a3-40a3-8e1e-28795cda8dcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.series.Series'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_training_steps, losses = train_model(model, text_train, train_y, text_test, test_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "589f7e245a894b829d19087da0fcae1c",
            "267537a8a87a456abd823ce59303f23b",
            "afb81b768b1c44e199dd739c2393a865",
            "ae578cf93f3a4b35b02be8bdbe696c89",
            "ca012c4121204b7fbb964f4a71224cda",
            "b932352b9655484093b519239800c724",
            "b1ae841ef67644828759f4c80d712102",
            "2b3f0a8541bf41d6afa35e3f0783d6df",
            "964d18ad8ebb48a3aec9b81b54cbdd51",
            "351ec7e793f5443a83867d104ae7ff52",
            "57263abdc4294a61b3a64b61c52e9d21"
          ]
        },
        "id": "G0Mcyj7LBXQV",
        "outputId": "00860e3d-1a60-4b2a-a12a-6f680e02a349"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Steps  10100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "589f7e245a894b829d19087da0fcae1c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5855    0.4299    0.4958      2198\n",
            "           1     0.3762    0.6091    0.4651      1522\n",
            "           2     0.0000    0.0000    0.0000       358\n",
            "\n",
            "    accuracy                         0.4590      4078\n",
            "   macro avg     0.3206    0.3463    0.3203      4078\n",
            "weighted avg     0.4560    0.4590    0.4408      4078\n",
            "\n",
            "Epoch:  2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5855    0.4299    0.4958      2198\n",
            "           1     0.3762    0.6091    0.4651      1522\n",
            "           2     0.0000    0.0000    0.0000       358\n",
            "\n",
            "    accuracy                         0.4590      4078\n",
            "   macro avg     0.3206    0.3463    0.3203      4078\n",
            "weighted avg     0.4560    0.4590    0.4408      4078\n",
            "\n",
            "Epoch:  3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5855    0.4299    0.4958      2198\n",
            "           1     0.3762    0.6091    0.4651      1522\n",
            "           2     0.0000    0.0000    0.0000       358\n",
            "\n",
            "    accuracy                         0.4590      4078\n",
            "   macro avg     0.3206    0.3463    0.3203      4078\n",
            "weighted avg     0.4560    0.4590    0.4408      4078\n",
            "\n",
            "Epoch:  4\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-dff09756e5d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnum_training_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-48-744c575e1f0e>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, x_train, y_train, x_valid, y_valid)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m       \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/optimization.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"params\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1102\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "code.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "IMHPbMBI4VTQ",
        "7MBdfMwk4bR_",
        "2abJM4PN4yyY",
        "5_goD7gM5Bnj",
        "KsTCPqZS4jzh",
        "cWK3dbEM45Fj",
        "Kz3D-3YoDine",
        "lgvLSOR8gsOn",
        "tUiQnNNV5Frg"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "589f7e245a894b829d19087da0fcae1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_267537a8a87a456abd823ce59303f23b",
              "IPY_MODEL_afb81b768b1c44e199dd739c2393a865",
              "IPY_MODEL_ae578cf93f3a4b35b02be8bdbe696c89"
            ],
            "layout": "IPY_MODEL_ca012c4121204b7fbb964f4a71224cda"
          }
        },
        "267537a8a87a456abd823ce59303f23b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b932352b9655484093b519239800c724",
            "placeholder": "​",
            "style": "IPY_MODEL_b1ae841ef67644828759f4c80d712102",
            "value": " 17%"
          }
        },
        "afb81b768b1c44e199dd739c2393a865": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b3f0a8541bf41d6afa35e3f0783d6df",
            "max": 10100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_964d18ad8ebb48a3aec9b81b54cbdd51",
            "value": 1764
          }
        },
        "ae578cf93f3a4b35b02be8bdbe696c89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_351ec7e793f5443a83867d104ae7ff52",
            "placeholder": "​",
            "style": "IPY_MODEL_57263abdc4294a61b3a64b61c52e9d21",
            "value": " 1764/10100 [04:39&lt;19:03,  7.29it/s]"
          }
        },
        "ca012c4121204b7fbb964f4a71224cda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b932352b9655484093b519239800c724": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1ae841ef67644828759f4c80d712102": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b3f0a8541bf41d6afa35e3f0783d6df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "964d18ad8ebb48a3aec9b81b54cbdd51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "351ec7e793f5443a83867d104ae7ff52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57263abdc4294a61b3a64b61c52e9d21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}