{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanduerhan/Licenta/blob/main/code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OH7f6H-lQfOA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Import"
      ],
      "metadata": {
        "id": "EOmssdxm3o5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "RI7LFohhh6oy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66fcc8bf-bbd4-4f68-a921-a3bd80875328"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade numpy"
      ],
      "metadata": {
        "id": "BHx29AkXpS9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "ZX3Ibl5BQUFC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import openpyxl\n",
        "from nltk.corpus import stopwords, words, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "import gensim\n",
        "import re, string\n",
        "from string import punctuation, digits\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import one_hot,Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.initializers import Constant\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import ReLU\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense , Flatten ,Embedding,Input,LSTM\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "col_list = [\"Column2\", \"Column3\"]\n",
        "dataset = pd.read_excel(\"Training.data.xlsx\", usecols=col_list)\n",
        "testset = pd.read_excel(\"Dev.xlsx\", usecols=col_list)\n",
        "\n",
        "\n",
        "def add_category_id(dataset):\n",
        "  dataset['category_id'] = dataset['Column3'].factorize()[0]\n",
        "  category_id_dataset = dataset[['Column3', 'category_id']].drop_duplicates()\n",
        "\n",
        "  category_to_id = dict(category_id_dataset.values)\n",
        "  id_to_category = dict(category_id_dataset[['category_id', 'Column3']].values)\n",
        "  return dataset,category_to_id, id_to_category\n",
        "\n",
        "# adding numerical categories\n",
        "dataset, category_to_id_train, id_to_category_train = add_category_id(dataset)\n",
        "testset, category_to_id_test, id_to_category_test = add_category_id(testset)\n",
        "\n",
        "# Shuffling the datasets\n",
        "dataset = dataset.sample(frac = 1)\n",
        "testset = testset.sample(frac = 1)\n",
        "\n",
        "train_x = dataset[\"Column2\"]\n",
        "train_y = dataset[\"category_id\"]\n",
        "test_x = testset[\"Column2\"]\n",
        "test_y = testset[\"category_id\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_x[7412])"
      ],
      "metadata": {
        "id": "Yy4EFKrVeZaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a try to keep only unique tweets (not used in paper)\n",
        "\n",
        "train_tweets = dataset['Column2'].tolist()\n",
        "test_tweets = testset['Column2'].tolist()\n",
        "def keep_uniques(array, df):\n",
        "    dels=[]\n",
        "    for i in array:\n",
        "        if array.count(i)>1:\n",
        "            dels.append(i)\n",
        "    dels=list(set(dels))\n",
        "    for i in dels:\n",
        "        df.drop( df[ df['Column2'] == i ].index, inplace=True)\n",
        "    return df\n",
        "\n",
        "dataset = keep_uniques(train_tweets, dataset)\n",
        "testset = keep_uniques(test_tweets, testset)\n",
        "\n",
        "print(len(testset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_jdBb8Z_Pus",
        "outputId": "4bdd86af-ceb3-45a2-bd83-5498c1b58d84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fig = plt.figure(figsize=(8,6))\n",
        "colors = ['lightblue','blue','darkblue']\n",
        "\n",
        "train_mean = dataset.groupby('Column3').Column2.count().sort_values()\n",
        "print(train_mean)\n",
        "test_mean = testset.groupby('Column3').Column2.count().sort_values()\n",
        "X = ['Antrenare','Validare']\n",
        "severe_mean = [train_mean[0], test_mean[0]]\n",
        "not_mean = [train_mean[1], test_mean[1]]\n",
        "moderate_mean = [train_mean[2], test_mean[2]]\n",
        "X_axis = np.arange(len(X))\n",
        "\n",
        "plt.bar(X_axis , moderate_mean, 0.2, label = 'Moderat',color =\"darkblue\")\n",
        "plt.bar(X_axis + 0.2, not_mean, 0.2, label = 'Fără depresie',color = 'blue')\n",
        "plt.bar(X_axis + 0.2*2, severe_mean, 0.2, label = 'Sever',color = 'lightblue')\n",
        "  \n",
        "plt.xticks(X_axis, X)\n",
        "plt.ylabel(\"Număr de apariții\")\n",
        "plt.title(\"Tweeturi per categorie\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PQ4_yB2Cdnad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = ['SMOTE', 'Ponderi de Clase', 'Nebalansat']\n",
        "vec1 = [0.4224,0.4297, 0.3807]\n",
        "vec2 = [0.4375,0.4505,0.4275]\n",
        "vec3 = [0.4793,0.4643, 0.4612]\n",
        "vec4 = [0.4776,0.4690, 0.4441]\n",
        "X_axis = np.arange(3)\n",
        "\n",
        "plt.bar(X_axis , vec4, 0.2, color =\"darkblue\")\n",
        "plt.bar(X_axis + 0.2, vec3, 0.2, color = 'blue')\n",
        "plt.bar(X_axis + 0.2*2, vec2, 0.2, color = 'lightblue')\n",
        "plt.bar(X_axis + 0.2*3, vec1, 0.2, color = 'blue')\n",
        "plt.xticks(X_axis, X)\n",
        "plt.ylabel(\"Scor F1\")\n",
        "plt.title(\"Metode de balansare a datelor\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C1vHNHDjr4SA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PreProcessing"
      ],
      "metadata": {
        "id": "xHx-Yr8O3pvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tweet-preprocessor\n",
        "import preprocessor as p\n",
        "\n",
        "def preprocess_tweets(text):\n",
        "  return p.clean(text)\n",
        "\n",
        "print(preprocess_tweets(train_x[7412]))"
      ],
      "metadata": {
        "id": "i5ORHpDKp2eO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# cleaning the text : deleting the emojis, the existing emails, the punctuation, the present digits,\n",
        "# the hyperlinks and the stopwords (a,the,is, etc)\n",
        "\n",
        "def delete_emoji(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               u\"\\U0001f926-\\U0001f937\"\n",
        "                               u\"\\U00010000-\\U0010ffff\"\n",
        "                               u\"\\u2640-\\u2642\"\n",
        "                               u\"\\u2600-\\u2B55\"\n",
        "                               u\"\\u200d\"\n",
        "                               u\"\\u23cf\"\n",
        "                               u\"\\u23e9\"\n",
        "                               u\"\\u231a\"\n",
        "                               u\"\\ufe0f\"  # dingbats\n",
        "                               u\"\\u3030\"\n",
        "                               \"]+\", re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)  # no emoji\n",
        "\n",
        "\n",
        "def delete_digits(text):\n",
        "    text = text.lower()\n",
        "    clean = text.translate(str.maketrans('', '', digits))\n",
        "    return clean\n",
        "\n",
        "\n",
        "def delete_punctuation(text):\n",
        "    clean = text.translate(str.maketrans('', '', punctuation + '’“”'))\n",
        "    return clean\n",
        "\n",
        "\n",
        "stop = set(stopwords.words('english'))\n",
        "stop.add('gtpoplt')\n",
        "stop.add('new')\n",
        "stop.add('year')\n",
        "stop.add('eve')\n",
        "stop.add('years')\n",
        "stop.add('ti')\n",
        "stop.add('ame')\n",
        "stop.add('folks')\n",
        "punct = list(string.punctuation)\n",
        "stop.update(punct)\n",
        "\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    final_text = []\n",
        "    for i in text.split():\n",
        "        if i.strip().lower() not in stop:\n",
        "            final_text.append(i.strip())\n",
        "    return \" \".join(final_text)\n",
        "\n",
        "\n",
        "def remove_hyperlinks(text):\n",
        "    for word in text:\n",
        "        if re.match(r'^http', word):\n",
        "            text.remove(word)\n",
        "    return \" \".join(text)\n",
        "\n",
        "def remove_emails(text):\n",
        "    text = text.split()\n",
        "    for i in text:\n",
        "        if '@' in i.strip().lower():\n",
        "            text.remove(i)\n",
        "    return \" \".join(text)\n",
        "\n",
        "contraction_dict = {\"youre\":\"you are\",\"im\": \"i am\",\"wouldnt\": \"would not\",\"itll\": \"it will\",\"wasnt\": \"was not\",\"dont\": \"do not\",\"ill\": \"i will\",\"isnt\": \"is not\",\"cant\": \"cannot\",\"arent\": \"are not\",\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
        "def _get_contractions(contraction_dict):\n",
        "    contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n",
        "    return contraction_dict, contraction_re\n",
        "contractions, contractions_re = _get_contractions(contraction_dict)\n",
        "def replace_contractions(text):\n",
        "    def replace(match):\n",
        "        return contractions[match.group(0)]\n",
        "    return contractions_re.sub(replace, text)\n",
        "\n",
        "\n",
        "train_x = dataset[\"Column2\"]\n",
        "train_y = dataset[\"category_id\"]\n",
        "test_x = testset[\"Column2\"]\n",
        "test_y = testset[\"category_id\"]\n",
        "\n",
        "\n",
        "def denoise(train_x):\n",
        "  for i in range(len(train_x)):\n",
        "    train_x[i] = replace_contractions(train_x[i])\n",
        "    train_x[i] = remove_emails(train_x[i])\n",
        "    train_x[i] = delete_punctuation(train_x[i])\n",
        "    train_x[i] = delete_emoji(train_x[i])\n",
        "    train_x[i] = delete_digits(train_x[i])\n",
        "    train_x[i] = remove_stopwords(train_x[i])\n",
        "  return train_x\n",
        "\n",
        "train_x = denoise(train_x)\n",
        "test_x = denoise(test_x)\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "fySiVht3QUFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_x[449])"
      ],
      "metadata": {
        "id": "RfI378zkgRMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatization"
      ],
      "metadata": {
        "id": "cfS5G2jQ4GGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayod5PmNYU7H",
        "outputId": "eb65307a-4e4a-4f2c-f0a3-1366388466d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/omw-1.4.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# lemmatization : crying -> cry, days -> day with pos tagging\n",
        "\n",
        "def get_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "\n",
        "def lemm(text):\n",
        "    text = text.split()\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    sar_list_lemmatizer = [lemmatizer.lemmatize(word, get_pos(word)) for word in text]\n",
        "    return \" \".join(sar_list_lemmatizer)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "3oSwfDyzQUFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = train_x.apply(lemm)\n",
        "test_x = test_x.apply(lemm)"
      ],
      "metadata": {
        "id": "Zv21BaGJBhgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_x[7429])"
      ],
      "metadata": {
        "id": "0Gst2fP9GIvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "text = ' '.join(train_x[dataset['Column3']=='severe'])\n",
        "wc = WordCloud(background_color='white').generate(text)\n",
        "plt.imshow(wc)"
      ],
      "metadata": {
        "id": "t0Oao20MI9Mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Count Vectorizer"
      ],
      "metadata": {
        "id": "IMHPbMBI4VTQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "source": [
        "# CountVectorizer reprezentation for the user tweets\n",
        "def count_vec(train,test):\n",
        "  vectorizer = CountVectorizer()\n",
        "  x_train_cv = vectorizer.fit_transform(train)\n",
        "\n",
        "  x_test_cv = vectorizer.transform(test)\n",
        "  return x_train_cv, x_test_cv"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "8blt8KSfQUFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_cv, x_test_cv = count_vec(train_x, test_x)"
      ],
      "metadata": {
        "id": "XJK62wRaBvyu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data imbalance handling"
      ],
      "metadata": {
        "id": "7MBdfMwk4bR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sm = SMOTE(random_state = 42)\n",
        "res_x, res_y = sm.fit_resample(x_train_cv, train_y)\n",
        "print(\"After OverSampling, counts of label '2': {}\".format(sum(res_y == 2)))"
      ],
      "metadata": {
        "id": "w-x13nL-a0EM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "135d7aff-b35e-4dd2-c831-9afa4600461f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After OverSampling, counts of label '2': 5647\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os = RandomOverSampler()\n",
        "res_x2, res_y2 = os.fit_resample(x_train_cv, train_y)"
      ],
      "metadata": {
        "id": "020ZvbDV8Yk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tf-Idf "
      ],
      "metadata": {
        "id": "2abJM4PN4yyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tf-idf reprezentation for the tweets\n",
        "def tf_idf(train, test, y):\n",
        "  vectorizer = TfidfVectorizer(max_df = 0.15,min_df = 5, ngram_range=(1,2), stop_words='english')\n",
        "  train_x_tf = vectorizer.fit_transform(train)\n",
        "  test_x_tf = vectorizer.transform(test)\n",
        "\n",
        "  res_tfx, res_tfy = sm.fit_resample(train_x_tf, y)\n",
        "  return train_x_tf, test_x_tf, res_tfx, res_tfy"
      ],
      "metadata": {
        "id": "WkoI7tMeRnXz"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x_tf, test_x_tf, res_tfx, res_tfy = tf_idf(train_x, test_x, train_y)"
      ],
      "metadata": {
        "id": "AkqK0TxgB_k9"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word2Vec"
      ],
      "metadata": {
        "id": "5_goD7gM5Bnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent = [row.split() for row in train_x]"
      ],
      "metadata": {
        "id": "Q_TNLIZYk8DA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.phrases import Phrases, Phraser"
      ],
      "metadata": {
        "id": "wH95fWT2lHE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram = Phraser(phrases)\n",
        "sentences = bigram[sent]"
      ],
      "metadata": {
        "id": "ffE5uhQflNF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentences)"
      ],
      "metadata": {
        "id": "xslKOh8bnezF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model = Word2Vec(min_count=20,\n",
        "                     window=5,\n",
        "                     size=300,\n",
        "                     sample=6e-5, \n",
        "                     alpha=0.03, \n",
        "                     min_alpha=0.0007, \n",
        "                     negative=20,\n",
        "                     )"
      ],
      "metadata": {
        "id": "XP-cfBanlg4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.build_vocab(sentences)"
      ],
      "metadata": {
        "id": "FjD343pblz0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)"
      ],
      "metadata": {
        "id": "uHmC66qBl6T1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.wv.most_similar(positive=[\"depression\"])"
      ],
      "metadata": {
        "id": "rSI6JFcmmM43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model,word_index,pad_rev_train = word2(train_x)"
      ],
      "metadata": {
        "id": "eJRUq5f6v1FO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2,word_index2,pad_rev_test = word2(test_x)"
      ],
      "metadata": {
        "id": "kwKmyoLL0HF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_weight_matrix(model, vocab):\n",
        "    vocab_size = len(vocab) + 1\n",
        "    weight_matrix = np.zeros((vocab_size, 300))\n",
        "    for word, i in vocab.items():\n",
        "        weight_matrix[i] = model[word]\n",
        "    return weight_matrix"
      ],
      "metadata": {
        "id": "O25ga7HBxIk5"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_vectors = get_weight_matrix(model, word_index)"
      ],
      "metadata": {
        "id": "XBosyRN8xMkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def w2v_embedding(train_text):\n",
        "\n",
        "  lst_corpus = []\n",
        "  for string in train_text:\n",
        "    lst_words = string.split()\n",
        "    lst_grams = [\" \".join(lst_words[i:i+1]) \n",
        "                for i in range(0, len(lst_words), 1)]\n",
        "    lst_corpus.append(lst_grams)\n",
        "\n",
        "  bigrams_detector = gensim.models.phrases.Phrases(lst_corpus, \n",
        "                  delimiter=\" \".encode(), min_count=30)\n",
        "  bigrams_detector = gensim.models.phrases.Phraser(bigrams_detector)\n",
        "  trigrams_detector = gensim.models.phrases.Phrases(bigrams_detector[lst_corpus], \n",
        "              delimiter=\" \".encode(), min_count=30)\n",
        "  trigrams_detector = gensim.models.phrases.Phraser(trigrams_detector)\n",
        "\n",
        "  nlp = Word2Vec(lst_corpus, size=300,   \n",
        "            window=8, min_count=2, sg=1, iter=30)\n",
        "  return nlp, lst_corpus, bigrams_detector, trigrams_detector"
      ],
      "metadata": {
        "id": "YqW8Yiqr4vEL"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_engineering(lst_corpus, train_text):\n",
        "  tokenizer = Tokenizer(lower=True, split=' ', \n",
        "                     oov_token=\"NaN\", \n",
        "                     filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
        "  tokenizer.fit_on_texts(lst_corpus)\n",
        "  dic_vocabulary = tokenizer.word_index\n",
        "\n",
        "  lst_corpus = []\n",
        "  for string in train_text:\n",
        "    lst_words = string.split()\n",
        "    lst_grams = [\" \".join(lst_words[i:i+1]) \n",
        "                for i in range(0, len(lst_words), 1)]\n",
        "    lst_corpus.append(lst_grams)\n",
        "  lst_text2seq= tokenizer.texts_to_sequences(lst_corpus)\n",
        "\n",
        "  X_train = pad_sequences(lst_text2seq, \n",
        "                      maxlen=30, padding=\"post\", truncating=\"post\")\n",
        "  return tokenizer, dic_vocabulary, X_train"
      ],
      "metadata": {
        "id": "HXONvgmg40qr"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_embedding_matrix(dic_vocabulary, nlp):\n",
        "  embeddings = np.zeros((len(dic_vocabulary)+1, 300))\n",
        "  for word,idx in dic_vocabulary.items():\n",
        "      try:\n",
        "          embeddings[idx] =  nlp[word]\n",
        "      except:\n",
        "          pass\n",
        "  return embeddings"
      ],
      "metadata": {
        "id": "w3XdYm2A421r"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp, lst_corpus, bigrams_detector, trigrams_detector = w2v_embedding(train_x)"
      ],
      "metadata": {
        "id": "eYbWGZel4488"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(nlp.wv.most_similar(positive=\"depression\"))"
      ],
      "metadata": {
        "id": "5WIjrfeL9fYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer, dic_vocabulary, X_train = feature_engineering(lst_corpus,train_x)"
      ],
      "metadata": {
        "id": "4O1vFy469Jaf"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)"
      ],
      "metadata": {
        "id": "RYBf8fPNBzrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_train = make_embedding_matrix(dic_vocabulary, nlp)"
      ],
      "metadata": {
        "id": "l3JJiPOE9QLZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "598e5368-a2bc-4b93-b15a-7f33b1408ea5"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \"\"\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_handling(test_text,bigrams_detector, trigrams_detector, tokenizer):\n",
        "\n",
        "  lst_corpus = []\n",
        "  for string in test_text:\n",
        "      lst_words = string.split()\n",
        "      lst_grams = [\" \".join(lst_words[i:i+1]) for i in range(0, \n",
        "                  len(lst_words), 1)]\n",
        "      lst_corpus.append(lst_grams)\n",
        "\n",
        "  lst_corpus = list(bigrams_detector[lst_corpus])\n",
        "  lst_corpus = list(trigrams_detector[lst_corpus])\n",
        "\n",
        "  lst_text2seq = tokenizer.texts_to_sequences(lst_corpus)\n",
        "\n",
        "  X_test = pad_sequences(lst_text2seq, maxlen=30,\n",
        "              padding=\"post\", truncating=\"post\")\n",
        "  return X_test"
      ],
      "metadata": {
        "id": "iH5BAwFM_F8Y"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp2, lst_corpus2, bigrams_detector2, trigrams_detector2 = w2v_embedding(test_x)"
      ],
      "metadata": {
        "id": "JGH7zCw3C3mA"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = test_handling(test_x, bigrams_detector2, trigrams_detector2, tokenizer)"
      ],
      "metadata": {
        "id": "P7S9fXX7-E3v"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_test.shape)"
      ],
      "metadata": {
        "id": "IH1zz40QCuUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes"
      ],
      "metadata": {
        "id": "KsTCPqZS4jzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def naive_bayes(train_x, train_y, test_x, test_y):\n",
        "  naive_bayes_classifier = MultinomialNB()\n",
        "  naive_bayes_classifier.fit(train_x, train_y)\n",
        "  pred_y = naive_bayes_classifier.predict(test_x)\n",
        "\n",
        "  score1 = metrics.accuracy_score(test_y, pred_y)\n",
        "  print(\"Accuracy \" + str(score1))\n",
        "  print(metrics.classification_report(test_y, pred_y, digits = 4))"
      ],
      "metadata": {
        "id": "uDe2Ts7ZgnZs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "naive_bayes(res_x, res_y, x_test_cv, test_y)"
      ],
      "metadata": {
        "id": "o4k1tyGnC79-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77a29139-f53c-4ebe-e575-99c2ed426b85"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy 0.5272192251103482\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5720    0.8353    0.6790      2198\n",
            "           1     0.6351    0.1498    0.2424      1522\n",
            "           2     0.1690    0.2402    0.1984       358\n",
            "\n",
            "    accuracy                         0.5272      4078\n",
            "   macro avg     0.4587    0.4084    0.3733      4078\n",
            "weighted avg     0.5601    0.5272    0.4739      4078\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "naive_bayes(x_train_cv, train_y, x_test_cv, test_y)"
      ],
      "metadata": {
        "id": "5KhMSw_4DQi8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab121c49-b594-4cef-cff0-d2436d2b4837"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy 0.5259931338891614\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5739    0.8130    0.6728      2198\n",
            "           1     0.5497    0.1380    0.2206      1522\n",
            "           2     0.2543    0.4134    0.3149       358\n",
            "\n",
            "    accuracy                         0.5260      4078\n",
            "   macro avg     0.4593    0.4548    0.4028      4078\n",
            "weighted avg     0.5368    0.5260    0.4726      4078\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "naive_bayes(res_tfx, res_tfy, test_x_tf, test_y)"
      ],
      "metadata": {
        "id": "bLuvDkoGDBGf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a26ad43-ad13-4b97-a5c3-35cf8511afe2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy 0.280039234919078\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5653    0.2798    0.3743      2198\n",
            "           1     0.6378    0.1353    0.2233      1522\n",
            "           2     0.1204    0.8966    0.2122       358\n",
            "\n",
            "    accuracy                         0.2800      4078\n",
            "   macro avg     0.4411    0.4373    0.2700      4078\n",
            "weighted avg     0.5533    0.2800    0.3037      4078\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "naive_bayes(train_x_tf, train_y, test_x_tf, test_y)"
      ],
      "metadata": {
        "id": "n8bhAxIODG7u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5fb56ed-fbc9-4e1b-adfc-67990c23d0fe"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy 0.5399705738106915\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5395    1.0000    0.7009      2198\n",
            "           1     1.0000    0.0020    0.0039      1522\n",
            "           2     1.0000    0.0028    0.0056       358\n",
            "\n",
            "    accuracy                         0.5400      4078\n",
            "   macro avg     0.8465    0.3349    0.2368      4078\n",
            "weighted avg     0.7518    0.5400    0.3797      4078\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AdaBoost"
      ],
      "metadata": {
        "id": "cWK3dbEM45Fj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def adaboost(train_x, train_y, test_x, test_y):\n",
        "  ada = AdaBoostClassifier()\n",
        "  boost = ada.fit(train_x, train_y)\n",
        "  pred_y = boost.predict(test_x)\n",
        "  print(\"AdaBoost Classifier Model Accuracy:\", accuracy_score(test_y, pred_y))\n",
        "  print(metrics.classification_report(test_y, pred_y))"
      ],
      "metadata": {
        "id": "6QDGISapwPVO"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adaboost(res_x, res_y, x_test_cv, test_y)"
      ],
      "metadata": {
        "id": "tI9SyLLFDopm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b56e8965-6dfe-4bd6-caca-e003938be619"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Classifier Model Accuracy: 0.4492398234428642\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.48      0.54      2198\n",
            "           1       0.54      0.41      0.46      1522\n",
            "           2       0.12      0.41      0.19       358\n",
            "\n",
            "    accuracy                           0.45      4078\n",
            "   macro avg       0.43      0.43      0.40      4078\n",
            "weighted avg       0.55      0.45      0.48      4078\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adaboost(x_train_cv, train_y, x_test_cv, test_y)"
      ],
      "metadata": {
        "id": "_WdD3RTFDptd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93b62884-a924-47d0-b2b8-ecdd8033cb16"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Classifier Model Accuracy: 0.5223148602256008\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.90      0.68      2198\n",
            "           1       0.43      0.05      0.09      1522\n",
            "           2       0.32      0.25      0.28       358\n",
            "\n",
            "    accuracy                           0.52      4078\n",
            "   macro avg       0.43      0.40      0.35      4078\n",
            "weighted avg       0.48      0.52      0.42      4078\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adaboost(res_tfx, res_tfy, test_x_tf, test_y)"
      ],
      "metadata": {
        "id": "7AXgolmBDp5W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6a07436-f1af-4395-d919-37f4829e4389"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Classifier Model Accuracy: 0.4521824423737126\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.38      0.46      2198\n",
            "           1       0.48      0.55      0.51      1522\n",
            "           2       0.20      0.48      0.28       358\n",
            "\n",
            "    accuracy                           0.45      4078\n",
            "   macro avg       0.42      0.47      0.42      4078\n",
            "weighted avg       0.50      0.45      0.46      4078\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adaboost(train_x_tf, train_y, test_x_tf, test_y)"
      ],
      "metadata": {
        "id": "1Exgj1HjDp-4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba4edc1b-8e97-421e-add9-c85717cfe55e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Classifier Model Accuracy: 0.5122609122118685\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.86      0.67      2198\n",
            "           1       0.53      0.04      0.08      1522\n",
            "           2       0.27      0.35      0.31       358\n",
            "\n",
            "    accuracy                           0.51      4078\n",
            "   macro avg       0.45      0.42      0.35      4078\n",
            "weighted avg       0.51      0.51      0.42      4078\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression"
      ],
      "metadata": {
        "id": "Kz3D-3YoDine"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_weight = {0: 1.,\n",
        "                1: 3.,\n",
        "                2: 4.}\n",
        "\n",
        "def logistic_regression(train_x, train_y, test_x, test_y):\n",
        "  model = LogisticRegression(multi_class='multinomial', solver='newton-cg')\n",
        "\n",
        "  model = model.fit(train_x, train_y)\n",
        "  y_pred = model.predict(test_x)\n",
        "\n",
        "  print(\"Regression Classifier Model Accuracy:\", accuracy_score(test_y, y_pred))\n",
        "  print(metrics.classification_report(test_y, y_pred, digits = 4))\n",
        "\n",
        "  conf_mat = confusion_matrix(test_y, y_pred)\n",
        "  print(conf_mat)\n"
      ],
      "metadata": {
        "id": "hKYzd_kPDmDu"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_regression(res_x, res_y, x_test_cv, test_y)"
      ],
      "metadata": {
        "id": "ObN_RomsEE_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_regression(x_train_cv, train_y, x_test_cv, test_y)"
      ],
      "metadata": {
        "id": "tHnCLO2AEGwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_regression(res_tfx, res_tfy, test_x_tf, test_y)"
      ],
      "metadata": {
        "id": "oS3nHbzAEG0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_regression(train_x_tf, train_y, test_x_tf, test_y)"
      ],
      "metadata": {
        "id": "JdqY8HsYEG6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVM"
      ],
      "metadata": {
        "id": "6a-WtjG_Eilc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class_weight = {0: 1.,\n",
        "                1: 3.,\n",
        "                2: 6.}\n",
        "\n",
        "def svm(train_x, train_y, test_x, test_y):\n",
        "  model = LinearSVC()\n",
        "  model = model.fit(train_x_tf, train_y)\n",
        "  y_pred = model.predict(test_x_tf)\n",
        "  \n",
        "  print(\"Svc Classifier Model Accuracy:\", accuracy_score(test_y, y_pred))\n",
        "  print(metrics.classification_report(test_y, y_pred, digits = 4))\n",
        "\n",
        "  conf_mat = confusion_matrix(test_y, y_pred)\n",
        "  print(conf_mat)\n"
      ],
      "metadata": {
        "id": "wDRy1f9pv-I1"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm(res_x, res_y, x_test_cv, test_y)"
      ],
      "metadata": {
        "id": "5-1sZW-mEaNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm(x_train_cv, train_y, x_test_cv, test_y)"
      ],
      "metadata": {
        "id": "kbrYJSE8Eaqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm(res_tfx, res_tfy, test_x_tf, test_y)"
      ],
      "metadata": {
        "id": "MSAahaAvEat0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm(train_x_tf, train_y, test_x_tf, test_y)"
      ],
      "metadata": {
        "id": "gdlc9DW_Eazb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1287fc77-1d32-4b81-b853-d7881aa4052d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Svc Classifier Model Accuracy: 0.5465914664051006\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5866    0.7552    0.6603      2198\n",
            "           1     0.4787    0.2654    0.3415      1522\n",
            "           2     0.4084    0.4609    0.4331       358\n",
            "\n",
            "    accuracy                         0.5466      4078\n",
            "   macro avg     0.4912    0.4939    0.4783      4078\n",
            "weighted avg     0.5307    0.5466    0.5214      4078\n",
            "\n",
            "[[1660  388  150]\n",
            " [1029  404   89]\n",
            " [ 141   52  165]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# F1 metric"
      ],
      "metadata": {
        "id": "lgvLSOR8gsOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def f1(y_true, y_pred):\n",
        "    y_pred = K.round(y_pred)\n",
        "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
        "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
        "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
        "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
        "\n",
        "    p = tp / (tp + fp + K.epsilon())\n",
        "    r = tp / (tp + fn + K.epsilon())\n",
        "\n",
        "    f1 = 2*p*r / (p+r+K.epsilon())\n",
        "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
        "    return K.mean(f1)"
      ],
      "metadata": {
        "id": "bdQgAL5I6cCW"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f1_loss(y_true, y_pred):\n",
        "    \n",
        "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
        "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
        "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
        "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
        "\n",
        "    p = tp / (tp + fp + K.epsilon())\n",
        "    r = tp / (tp + fn + K.epsilon())\n",
        "\n",
        "    f1 = 2*p*r / (p+r+K.epsilon())\n",
        "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
        "    return 1 - K.mean(f1)"
      ],
      "metadata": {
        "id": "p3koGLPkDxnC"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network"
      ],
      "metadata": {
        "id": "tUiQnNNV5Frg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_weight = {0: 1.,\n",
        "                1: 3.,\n",
        "                2: 5.}\n",
        "# def split(pad_rev,train_y):\n",
        "#   Y=keras.utils.to_categorical(train_y)  # one hot target as required by NN.\n",
        "#   x_train,x_test,y_train,y_test=train_test_split(pad_rev,Y,test_size=0.05,random_state=42)\n",
        "#   return x_train, x_test,y_train,y_test\n",
        "def network(embeddings):\n",
        " ## input\n",
        "  x_in = layers.Input(shape=(30,))\n",
        "  ## embedding\n",
        "  x = layers.Embedding(input_dim=embeddings.shape[0],  \n",
        "                      output_dim=embeddings.shape[1], \n",
        "                      weights=[embeddings],\n",
        "                      input_length=30, trainable=False)(x_in)\n",
        "  ## 2 layers of bidirectional lstm\n",
        "  x = layers.Bidirectional(layers.LSTM(units=30, dropout=0.2, \n",
        "                          return_sequences=True))(x)\n",
        "  x = layers.Bidirectional(layers.LSTM(units=30, dropout=0.2))(x)\n",
        "  ## final dense layers\n",
        "  x = layers.Dense(64, activation='elu')(x)\n",
        "  y_out = layers.Dense(3, activation='softmax')(x)\n",
        "  ## compile\n",
        "  model = Model(x_in, y_out)\n",
        "  model.compile(loss=f1_loss,\n",
        "                optimizer='adam', metrics=[f1])\n",
        "  return model\n",
        "Y=keras.utils.to_categorical(train_y)\n",
        "Y2=keras.utils.to_categorical(test_y)\n",
        "# x_train,x_test,y_train,y_test = split(X_train,train_y)\n",
        "model = network(embeddings_train)\n",
        "# x_valid, x_x, y_valid, y_y = split(x_test,test_y)\n",
        "model.fit(X_train, Y, epochs=10, batch_size=64, validation_data=(x_test,Y2))\n"
      ],
      "metadata": {
        "id": "QAeWwiGyMJFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "Z922lilzZ5KH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_y = model.predict(x_test)\n",
        "pred_y = np.argmax(pred_y, axis=1)\n",
        "# y_test=np.argmax(y_test, axis=1)\n",
        "print(metrics.classification_report(test_y, pred_y, digits = 4))\n",
        "\n",
        "conf_mat = confusion_matrix(test_y, pred_y)\n",
        "print(conf_mat)"
      ],
      "metadata": {
        "id": "tHB9SVT5O4On"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT"
      ],
      "metadata": {
        "id": "fccPGYtiwkXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "hdP6B25p2WM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "OOyweo6YFNFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.initializers import GlorotNormal\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "gr1JKdNUF9yM"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizer"
      ],
      "metadata": {
        "id": "n_LErOdPgPJ5"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
      ],
      "metadata": {
        "id": "ptBxQVHFgah4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_train = train_x.astype(str).tolist()\n",
        "text_test = test_x.astype(str).tolist()"
      ],
      "metadata": {
        "id": "PsYlNr3RgtrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = tokenizer.tokenize(train_x)\n",
        "embt = tokenizer.encode(s)"
      ],
      "metadata": {
        "id": "A7u9Cgbygh0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizerFast, RobertaForSequenceClassification,Trainer, TrainingArguments"
      ],
      "metadata": {
        "id": "V7Ld9eDDhqt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RobertaForSequenceClassification.from_pretrained('roberta-base')\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base', max_length = 512)"
      ],
      "metadata": {
        "id": "XuvlQftshfwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence(texts):\n",
        "  model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
        "  embeddings = model.encode(texts)\n",
        "\n",
        "  return embeddings"
      ],
      "metadata": {
        "id": "IBHUesv8FbMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer(\"all-mpnet-base-v2\")"
      ],
      "metadata": {
        "id": "cz2XLEkFgy2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRANSFORMER_BATCH=128\n",
        "\n",
        "def count_embedd (df):\n",
        "    idx_chunk=list(df.columns).index('Column2')\n",
        "    embedd_lst = []\n",
        "    for index in range (0, len(df), TRANSFORMER_BATCH):\n",
        "        embedds = model.encode(df.iloc[index:index+TRANSFORMER_BATCH, idx_chunk].values, show_progress_bar=False)\n",
        "        embedd_lst.append(embedds)\n",
        "    return np.concatenate(embedd_lst)"
      ],
      "metadata": {
        "id": "HIZk6hn2aiu_"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_emb = count_embedd(dataset)"
      ],
      "metadata": {
        "id": "jyItDXSMgnog"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_emb = count_embedd(testset)"
      ],
      "metadata": {
        "id": "-4U0N7MSg4Wr"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.array(train_emb)\n",
        "X_test = np.array(test_emb)"
      ],
      "metadata": {
        "id": "CCkIOGTrbp2s"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train=keras.utils.to_categorical(train_y)"
      ],
      "metadata": {
        "id": "tf8ht-tgb_n_"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KERAS_VALIDATION_SPLIT=0.00\n",
        "KERAS_EPOCHS=10\n",
        "KERAS_BATCH_SIZE=128\n",
        "\n",
        "class_weight = {0: 1.,\n",
        "                1: 3.,\n",
        "                2: 6.}\n",
        "\n",
        "# Create and train Keras model\n",
        "n_features=X_train.shape[1]\n",
        "n_labels = y_train.shape[1]\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Dense(2048, input_dim=n_features),\n",
        "  tf.keras.layers.LeakyReLU(alpha=0.01),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(64),\n",
        "  tf.keras.layers.LeakyReLU(alpha=0.01),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "LR=5e-4\n",
        "adam = keras.optimizers.Adam(learning_rate=LR, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "\n",
        "model.compile(optimizer=adam, \n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[f1])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=KERAS_EPOCHS, batch_size=KERAS_BATCH_SIZE, validation_split=KERAS_VALIDATION_SPLIT)"
      ],
      "metadata": {
        "id": "HiKl8FLGb3ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()\n"
      ],
      "metadata": {
        "id": "ZZ2HfzDYboIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_preds = model.predict(X_test)\n",
        "predicted = np.argmax(y_preds,axis=1) \n",
        "accuracy = metrics.accuracy_score(test_y, predicted)\n",
        "print(\"Accuracy:\",  round(accuracy,4))\n",
        "print(metrics.classification_report(test_y, predicted,digits=4))\n",
        "\n",
        "conf_mat = confusion_matrix(test_y, predicted)\n",
        "print(conf_mat)"
      ],
      "metadata": {
        "id": "vyOx-aVqcLte"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "code.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "IMHPbMBI4VTQ",
        "7MBdfMwk4bR_",
        "2abJM4PN4yyY",
        "5_goD7gM5Bnj",
        "KsTCPqZS4jzh",
        "cWK3dbEM45Fj",
        "Kz3D-3YoDine",
        "lgvLSOR8gsOn",
        "tUiQnNNV5Frg"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}